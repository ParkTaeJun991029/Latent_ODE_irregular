{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import umap\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import SystemRandom\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "import torch.optim as optim\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.plotting import *\n",
    "\n",
    "from lib.rnn_baselines import *\n",
    "from lib.ode_rnn import *\n",
    "from lib.create_latent_ode_model import create_LatentODE_model\n",
    "from lib.parse_datasets import parse_datasets\n",
    "from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
    "from lib.diffeq_solver import DiffeqSolver\n",
    "from mujoco_physics import HopperPhysics\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.diffeq_solver import DiffeqSolver\n",
    "from generate_timeseries import Periodic_1d\n",
    "from torch.distributions import uniform\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from mujoco_physics import HopperPhysics\n",
    "from physionet import variable_time_collate_fn, get_data_min_max\n",
    "from person_activity import PersonActivity, variable_time_collate_fn_activity\n",
    "\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from lib.utils import compute_loss_all_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Number of iterations: 30000\n",
      "Learning rate: 0.01\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Generative model for noisy data based on ODE\n",
    "parser = argparse.ArgumentParser('Latent ODE')\n",
    "parser.add_argument('-n',  type=int, default=10000, help=\"Size of the dataset\")\n",
    "parser.add_argument('--niters', type=int, default=20)\n",
    "parser.add_argument('--lr',  type=float, default=1e-2, help=\"Starting learning rate.\")\n",
    "parser.add_argument('-b', '--batch-size', type=int, default=256)\n",
    "parser.add_argument('--viz', action='store_true', default=False, help=\"Show plots while training\")\n",
    "\n",
    "parser.add_argument('--save', type=str, default='experiments/', help=\"Path for save checkpoints\")\n",
    "parser.add_argument('--load', type=str, default=None, help=\"ID of the experiment to load for evaluation. If None, run a new experiment.\")\n",
    "parser.add_argument('-r', '--random-seed', type=int, default=1991, help=\"Random_seed\")\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default='physionet', help=\"Dataset to load. Available: physionet, activity, hopper, periodic\")\n",
    "parser.add_argument('-s', '--sample-tp', type=float, default=None, help=\"Number of time points to sub-sample.\"\n",
    "\t\"If > 1, subsample exact number of points. If the number is in [0,1], take a percentage of available points per time series. If None, do not subsample\")\n",
    "\n",
    "parser.add_argument('-c', '--cut-tp', type=int, default=None, help=\"Cut out the section of the timeline of the specified length (in number of points).\"\n",
    "\t\"Used for periodic function demo.\")\n",
    "\n",
    "parser.add_argument('--quantization', type=float, default=5, help=\"Quantization on the physionet dataset.\"\n",
    "\t\"Value 1 means quantization by 1 hour, value 0.1 means quantization by 0.1 hour = 6 min\")\n",
    "\n",
    "parser.add_argument('--latent-ode', action='store_true', default=True,  help=\"Run Latent ODE seq2seq model\")\n",
    "parser.add_argument('--z0-encoder', type=str, default='odernn', help=\"Type of encoder for Latent ODE model: odernn or rnn\")\n",
    "\n",
    "parser.add_argument('--classic-rnn', action='store_true', help=\"Run RNN baseline: classic RNN that sees true points at every point. Used for interpolation only.\")\n",
    "parser.add_argument('--rnn-cell', default=\"gru\", help=\"RNN Cell type. Available: gru (default), expdecay\")\n",
    "parser.add_argument('--input-decay', action='store_true', help=\"For RNN: use the input that is the weighted average of impirical mean and previous value (like in GRU-D)\")\n",
    "\n",
    "parser.add_argument('--ode-rnn', action='store_true', help=\"Run ODE-RNN baseline: RNN-style that sees true points at every point. Used for interpolation only.\")\n",
    "\n",
    "parser.add_argument('--rnn-vae', action='store_true', help=\"Run RNN baseline: seq2seq model with sampling of the h0 and ELBO loss.\")\n",
    "\n",
    "parser.add_argument('-l', '--latents', type=int, default=6, help=\"Size of the latent state\")\n",
    "parser.add_argument('--rec-dims', type=int, default=40, help=\"Dimensionality of the recognition model (ODE or RNN).\")\n",
    "\n",
    "parser.add_argument('--rec-layers', type=int, default=3, help=\"Number of layers in ODE func in recognition ODE\")\n",
    "parser.add_argument('--gen-layers', type=int, default=3, help=\"Number of layers in ODE func in generative ODE\")\n",
    "\n",
    "parser.add_argument('-u', '--units', type=int, default=50, help=\"Number of units per layer in ODE func\")\n",
    "parser.add_argument('-g', '--gru-units', type=int, default=100, help=\"Number of units per layer in each of GRU update networks\")\n",
    "\n",
    "parser.add_argument('--poisson', action='store_true', help=\"Model poisson-process likelihood for the density of events in addition to reconstruction.\")\n",
    "parser.add_argument('--classif', action='store_true', help=\"Include binary classification loss -- used for Physionet dataset for hospiral mortality\")\n",
    "\n",
    "parser.add_argument('--linear-classif', action='store_true', help=\"If using a classifier, use a linear classifier instead of 1-layer NN\")\n",
    "parser.add_argument('--extrap', action='store_true', help=\"Set extrapolation mode. If this flag is not set, run interpolation mode.\")\n",
    "\n",
    "parser.add_argument('-t', '--timepoints', type=int, default=100, help=\"Total number of time-points\")\n",
    "parser.add_argument('--max-t',  type=float, default=5., help=\"We subsample points in the interval [0, args.max_tp]\")\n",
    "parser.add_argument('--noise-weight', type=float, default=0.01, help=\"Noise amplitude for generated traejctories\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.n = 10000\n",
    "        self.niters = 1000\n",
    "        self.lr = 1e-2\n",
    "        self.batch_size = 128\n",
    "        self.viz = True\n",
    "        self.save = 'experiments/'\n",
    "        self.load = None\n",
    "        self.random_seed = 1991\n",
    "        self.dataset = 'physionet'\n",
    "        self.sample_tp = 0.6\n",
    "        self.cut_tp = None\n",
    "        self.quantization = 5\n",
    "        self.latent_ode = True\n",
    "        self.z0_encoder = 'odernn'\n",
    "        self.classic_rnn = False\n",
    "        self.rnn_cell = \"gru\"\n",
    "        self.input_decay = False\n",
    "        self.ode_rnn = False\n",
    "        self.rnn_vae = False\n",
    "        self.latents = 20\n",
    "        self.rec_dims = 10\n",
    "        self.rec_layers = 5\n",
    "        self.gen_layers = 5\n",
    "        self.units = 50\n",
    "        self.gru_units = 100\n",
    "        self.poisson = False\n",
    "        self.classif = False\n",
    "        self.linear_classif = False\n",
    "        self.extrap = False\n",
    "        self.timepoints = 100\n",
    "        self.max_t = 5.\n",
    "        self.noise_weight = 0.01\n",
    "\n",
    "# args 객체를 생성하고 필요한 설정을 할당합니다\n",
    "args = Args()\n",
    "\n",
    "# 몇 가지 설정을 수정합니다\n",
    "args.batch_size = 64\n",
    "args.classif = False\n",
    "args.quantization = 5\n",
    "args.niters = 30000\n",
    "args.n = 200\n",
    "args.sample_tp = None\n",
    "args.latents = 20\n",
    "args.rec_dims = 10\n",
    "args.rec_layers = 5\n",
    "args.gen_layers = 5\n",
    "args.latent_ode = True\n",
    "args.viz = True\n",
    "args.max_t = 200\n",
    "\n",
    "\n",
    "# 사용할 디바이스를 설정합니다\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 파일 이름을 설정합니다\n",
    "file_name = 'taejun_sim_'\n",
    "\n",
    "# 설정을 확인합니다\n",
    "print(\"Batch size:\", args.batch_size)\n",
    "print(\"Number of iterations:\", args.niters)\n",
    "print(\"Learning rate:\", args.lr)\n",
    "print(\"Device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Args at 0x7fe2cefebbe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from jdcal import jd2gcal\n",
    "from datetime import datetime\n",
    "\n",
    "class CustomClass(object):\n",
    "    params = ['Magnitude']  # Uncertainty_of_Magnitude 제외\n",
    "\n",
    "    params_dict = {k: i for i, k in enumerate(params)}\n",
    "\n",
    "    def __init__(self, root, train=True, preprocess=False,\n",
    "                 quantization=1, n_samples=None, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.quantization = quantization\n",
    "        self.device = device\n",
    "        self.reduce = \"average\"\n",
    "        if preprocess:\n",
    "            self.preprocess()\n",
    "\n",
    "        self.data = torch.load(os.path.join(self.root, 'lc_' + str(self.quantization) + '.pt'))\n",
    "        self.labels = torch.zeros(len(self.data))\n",
    "\n",
    "        # if n_samples is not None:\n",
    "        #     self.data = self.data[:n_samples]\n",
    "        #     self.labels = self.labels[:n_samples]\n",
    "\n",
    "    def preprocess(self):\n",
    "        simulation_data_root = self.root\n",
    "        data_list = [f for f in os.listdir(simulation_data_root) if f.endswith('.txt')]\n",
    "        data_list.sort()\n",
    "        \n",
    "        # Process only up to 1000 files\n",
    "        data_list = data_list[:1000]\n",
    "        \n",
    "        light_curves = []\n",
    "\n",
    "        for name in tqdm(data_list):\n",
    "            lc_name = name.partition('.')[0]\n",
    "            file_path = os.path.join(simulation_data_root, name)\n",
    "            \n",
    "            # Step 1: Compute max and min values\n",
    "            with open(file_path) as f:\n",
    "                next(f)  # Skip header\n",
    "                magnitudes = [float(line.rstrip().split(' ')[1]) for line in f]\n",
    "                max_magnitude = max(magnitudes)\n",
    "                min_magnitude = min(magnitudes)\n",
    "            \n",
    "            # Step 2: Process the file again to normalize and store data\n",
    "            with open(file_path) as f:\n",
    "                next(f)  # Skip header again\n",
    "                lines = f.readlines()\n",
    "                prev_time = 0\n",
    "                tt = [0.]\n",
    "                vals = [torch.zeros(len(self.params), device=self.device)]\n",
    "                mask = [torch.zeros(len(self.params), device=self.device)]\n",
    "                for line in lines:\n",
    "                    time, magnitude = line.rstrip().split(' ')[:2]\n",
    "                    time = float(time)\n",
    "                    magnitude = float(magnitude)\n",
    "\n",
    "                    # Normalize the magnitude\n",
    "                    normalized_magnitude = (magnitude - min_magnitude) / (max_magnitude - min_magnitude)\n",
    "\n",
    "                    if time != prev_time:\n",
    "                        tt.append(time)\n",
    "                        vals.append(torch.zeros(len(self.params), device=self.device))\n",
    "                        mask.append(torch.zeros(len(self.params), device=self.device))\n",
    "                        prev_time = time\n",
    "\n",
    "                    vals[-1][0] = normalized_magnitude  # Use normalized value\n",
    "                    mask[-1][0] = 1\n",
    "\n",
    "            tt = torch.tensor(tt, device=self.device)[1:]\n",
    "            vals = torch.stack(vals)[1:]\n",
    "            mask = torch.stack(mask)[1:]\n",
    "            labels = None\n",
    "            light_curves.append((lc_name, tt, vals, mask, labels))\n",
    "\n",
    "        torch.save(light_curves, os.path.join(self.root, 'lc_' + str(self.quantization) + '.pt'))\n",
    "        print('Done!')\n",
    "\n",
    "\n",
    "    # def preprocess(self):\n",
    "    #     simulation_data_root = self.root \n",
    "    #     data_list = [f for f in os.listdir(simulation_data_root) if f.endswith('.txt')]\n",
    "    #     data_list.sort()\n",
    "    #     light_curves = []\n",
    "\n",
    "    #     # 전체 데이터셋에 대한 최소값과 최대값을 초기화\n",
    "    #     global_min = float('inf')\n",
    "    #     global_max = float('-inf')\n",
    "\n",
    "    #     # 최소값과 최대값 찾기\n",
    "    #     for name in tqdm(data_list):\n",
    "    #         with open(os.path.join(simulation_data_root, name)) as f:\n",
    "    #             next(f)  # 헤더 건너뛰기\n",
    "    #             for line in f:\n",
    "    #                 _, magnitude = line.rstrip().split(' ')[:2]\n",
    "    #                 magnitude = float(magnitude)\n",
    "    #                 global_min = min(global_min, magnitude)\n",
    "    #                 global_max = max(global_max, magnitude)\n",
    "\n",
    "    #     # 데이터 스케일링 및 처리\n",
    "    #     for name in tqdm(data_list):\n",
    "    #         lc_name = name.partition('.')[0]\n",
    "    #         with open(os.path.join(simulation_data_root, name)) as f:\n",
    "    #             next(f)\n",
    "    #             lines = f.readlines()\n",
    "    #             prev_time = 0\n",
    "    #             tt = [0.]\n",
    "    #             vals = [torch.zeros(len(self.params), device=self.device)]\n",
    "    #             mask = [torch.zeros(len(self.params), device=self.device)]\n",
    "    #             for line in lines:\n",
    "    #                 time, magnitude = line.rstrip().split(' ')[:2]\n",
    "    #                 time = float(time)\n",
    "    #                 magnitude = float(magnitude)\n",
    "\n",
    "    #                 # 스케일링: (magnitude - global_min) / (global_max - global_min)\n",
    "    #                 scaled_magnitude = (magnitude - global_min) / (global_max - global_min)\n",
    "\n",
    "    #                 if time != prev_time:\n",
    "    #                     tt.append(time)\n",
    "    #                     vals.append(torch.zeros(len(self.params), device=self.device))\n",
    "    #                     mask.append(torch.zeros(len(self.params), device=self.device))\n",
    "    #                     prev_time = time\n",
    "\n",
    "    #                 vals[-1][0] = scaled_magnitude  # 스케일된 값 사용\n",
    "    #                 mask[-1][0] = 1\n",
    "\n",
    "    #         tt = torch.tensor(tt, device=self.device)[1:]\n",
    "    #         vals = torch.stack(vals)[1:]\n",
    "    #         mask = torch.stack(mask)[1:]\n",
    "    #         labels = None\n",
    "    #         light_curves.append((lc_name, tt, vals, mask, labels))\n",
    "\n",
    "    #     torch.save(light_curves, os.path.join(self.root, 'lc_' + str(self.quantization) + '.pt'))\n",
    "    #     print('Done!')\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        fmt_str += '    Split: {}\\n'.format('train' if self.train else 'test')\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        fmt_str += '    Quantization: {}\\n'.format(self.quantization)\n",
    "        fmt_str += '    Reduce: {}\\n'.format(self.reduce)\n",
    "        return fmt_str\n",
    "\n",
    "    def visualize(self, timesteps, data, mask, plot_name):\n",
    "        width = 15\n",
    "        height = 15\n",
    "        timesteps.to\n",
    "        non_zero_attributes = (torch.sum(mask,0) > 2).numpy()\n",
    "        non_zero_idx = [i for i in range(len(non_zero_attributes)) if non_zero_attributes[i] == 1.]\n",
    "        n_non_zero = sum(non_zero_attributes)\n",
    "\n",
    "        mask = mask[:, non_zero_idx]\n",
    "        data = data[:, non_zero_idx]\n",
    "\n",
    "        params_non_zero = [self.params[i] for i in non_zero_idx]\n",
    "        params_dict = {k: i for i, k in enumerate(params_non_zero)}\n",
    "\n",
    "        n_col = 3\n",
    "        n_row = n_non_zero // n_col + (n_non_zero % n_col > 0)\n",
    "        fig, ax_list = plt.subplots(n_row, n_col, figsize=(width, height), facecolor='white')\n",
    "\n",
    "        #for i in range(len(self.params)):\n",
    "        for i in range(n_non_zero):\n",
    "            param = params_non_zero[i]\n",
    "            param_id = params_dict[param]\n",
    "\n",
    "            tp_mask = mask[:,param_id].long()\n",
    "\n",
    "            tp_cur_param = timesteps[tp_mask == 1.]\n",
    "            data_cur_param = data[tp_mask == 1., param_id]\n",
    "\n",
    "            ax_list[i // n_col, i % n_col].plot(tp_cur_param.numpy(), data_cur_param.numpy(),  marker='o') \n",
    "            ax_list[i // n_col, i % n_col].set_title(param)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(plot_name)\n",
    "        plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드에 구현된거 하지만 잘 안된다 수정해서 해보자\n",
    "def variable_time_collate_fn(batch, args, device = device, data_type = \"train\", \n",
    "\tdata_min = None, data_max = None):\n",
    "\t\"\"\"\n",
    "\tExpects a batch of time series data in the form of (record_id, tt, vals, mask, labels) where\n",
    "\t\t- record_id is a patient id\n",
    "\t\t- tt is a 1-dimensional tensor containing T time values of observations.\n",
    "\t\t- vals is a (T, D) tensor containing observed values for D variables.\n",
    "\t\t- mask is a (T, D) tensor containing 1 where values were observed and 0 otherwise.\n",
    "\t\t- labels is a list of labels for the current patient, if labels are available. Otherwise None.\n",
    "\tReturns:\n",
    "\t\tcombined_tt: The union of all time observations.\n",
    "\t\tcombined_vals: (M, T, D) tensor containing the observed values.\n",
    "\t\tcombined_mask: (M, T, D) tensor containing 1 where values were observed and 0 otherwise.\n",
    "\t\"\"\"\n",
    "\tD = batch[0][2].shape[1]\n",
    "\tcombined_tt, inverse_indices = torch.unique(torch.cat([ex[1] for ex in batch]), sorted=True, return_inverse=True)\n",
    "\tcombined_tt = combined_tt.to(device)\n",
    "\n",
    "\toffset = 0\n",
    "\tcombined_vals = torch.zeros([len(batch), len(combined_tt), D]).to(device)\n",
    "\tcombined_mask = torch.zeros([len(batch), len(combined_tt), D]).to(device)\n",
    "\t\n",
    "\tcombined_labels = None\n",
    "\tN_labels = 1\n",
    "\n",
    "\tcombined_labels = torch.zeros(len(batch), N_labels) + torch.tensor(float('nan'))\n",
    "\tcombined_labels = combined_labels.to(device = device)\n",
    "\t\n",
    "\tfor b, (record_id, tt, vals, mask, labels) in enumerate(batch):\n",
    "\t\ttt = tt.to(device)\n",
    "\t\tvals = vals.to(device)\n",
    "\t\tmask = mask.to(device)\n",
    "\t\tif labels is not None:\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\tindices = inverse_indices[offset:offset + len(tt)]\n",
    "\t\toffset += len(tt)\n",
    "\n",
    "\t\tcombined_vals[b, indices] = vals\n",
    "\t\tcombined_mask[b, indices] = mask\n",
    "\n",
    "\t\tif labels is not None:\n",
    "\t\t\tcombined_labels[b] = labels\n",
    "\n",
    "\tcombined_vals, _, _ = utils.normalize_masked_data(combined_vals, combined_mask, \n",
    "\t \tatt_min = data_min, att_max = data_max)\n",
    "\n",
    "\tif torch.max(combined_tt) != 0.:\n",
    "\t\tcombined_tt = combined_tt / torch.max(combined_tt)\n",
    "\t\n",
    "\tdata_dict = {\n",
    "\t\t\"data\": combined_vals, \n",
    "\t\t\"time_steps\": combined_tt,\n",
    "\t\t\"mask\": combined_mask,\n",
    "\t\t\"labels\": combined_labels}\n",
    "\t\n",
    "\tdata_dict = utils.split_and_subsample_batch(data_dict, args, data_type = data_type)\n",
    "\treturn data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:11<00:00, 17.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset_obj = CustomClass(root='/home/intern/SSD/intern/taejun/data_normal', train=True, preprocess=True,quantization=5,n_samples = min(800, args.n), device=device)\n",
    "\t\t# Use custom collate_fn to combine samples with arbitrary time observations.\n",
    "\t\t# Returns the dataset along with mask and time steps\n",
    "\n",
    "\n",
    "# Combine and shuffle samples from physionet Train and physionet Test\n",
    "total_dataset = train_dataset_obj[:len(train_dataset_obj)]\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle and split\n",
    "train_data, test_data = model_selection.train_test_split(total_dataset, train_size= 0.8, \n",
    "    random_state = 42, shuffle = True)\n",
    "\n",
    "record_id, tt, vals, mask, labels = train_data[0]\n",
    "\n",
    "n_samples = len(total_dataset)\n",
    "input_dim = vals.size(-1)\n",
    "\n",
    "batch_size = min(min(len(train_dataset_obj), args.batch_size), args.n)\n",
    "data_min, data_max = get_data_min_max(total_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size= batch_size, shuffle=False, \n",
    "    collate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"train\",\n",
    "        data_min = data_min, data_max = data_max))\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size, shuffle=False, \n",
    "    collate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"train\",\n",
    "        data_min = data_min, data_max = data_max))\n",
    "\n",
    "attr_names = train_dataset_obj.params\n",
    "data_objects = {\"dataset_obj\": train_dataset_obj, \n",
    "            \"train_dataloader\": utils.inf_generator(train_dataloader), \n",
    "            \"test_dataloader\": utils.inf_generator(test_dataloader),\n",
    "            \"input_dim\": input_dim,\n",
    "            \"n_train_batches\": len(train_dataloader),\n",
    "            \"n_test_batches\": len(test_dataloader),\n",
    "            \"attr\": attr_names, #optional\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.sample_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt_file_path = '/home/intern/SSD/intern/taejun/data_normal/lc_5.pt'\n",
    "\n",
    "# # .pt 파일 로드\n",
    "# light_curves_data = torch.load(pt_file_path)\n",
    "\n",
    "# # 로드된 데이터의 타입과 크기 확인\n",
    "# print(f\"Loaded data type: {type(light_curves_data)}\")\n",
    "# print(f\"Number of light curves in the dataset: {len(light_curves_data)}\")\n",
    "\n",
    "# # 첫 번째 광도곡선 데이터의 구조 확인\n",
    "# first_light_curve = light_curves_data[0]\n",
    "# print(f\"Structure of a single light curve data: {type(first_light_curve)}\")\n",
    "# print(f\"Record ID: {first_light_curve[0]}\")\n",
    "# print(f\"Time stamps tensor shape: {first_light_curve[1].shape}\")\n",
    "# print(f\"Magnitude tensor shape: {first_light_curve[2].shape}\")\n",
    "# print(f\"Mask tensor shape: {first_light_curve[3].shape}\")\n",
    "# #print(f\"Labels tensor shape: {first_light_curve[4].shape}\")\n",
    "\n",
    "# # 첫 번째 광도곡선 데이터의 일부 내용 출력\n",
    "# print(\"\\nSample data from the first light curve:\")\n",
    "# print(f\"Time stamps: {first_light_curve[1]}\")  # 처음 5개의 시간 스탬프\n",
    "# print(f\"Magnitude: {first_light_curve[2]}\")  # 처음 5개의 광도 값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader:\n",
      "Batch 1:\n",
      "observed_data.shape: torch.Size([64, 49, 1])\n",
      "data_to_predict: torch.Size([64, 49, 1])\n",
      "Batch 2:\n",
      "observed_data.shape: torch.Size([64, 49, 1])\n",
      "data_to_predict: torch.Size([64, 49, 1])\n",
      "Batch 3:\n",
      "observed_data.shape: torch.Size([32, 49, 1])\n",
      "data_to_predict: torch.Size([32, 49, 1])\n"
     ]
    }
   ],
   "source": [
    "# `train_dataloader`에서 첫 번째 배치를 가져와서 형태와 내용을 확인합니다.\n",
    "print(\"Train DataLoader:\")\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(\"observed_data.shape:\", batch['observed_data'].shape)\n",
    "    #print(\"observed_data:\", batch['observed_data'])\n",
    "    print(\"data_to_predict:\", batch['data_to_predict'].shape)\n",
    "    #여기에서 필요한 다른 키들도 확인할 수 있습니다.\n",
    "    # 예를\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory where you want to save the file\n",
    "directory = \"experiments\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intern/anaconda3/envs/taejun/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:296: UserWarning: t is not on the same device as y0. Coercing to y0.device.\n",
      "  warnings.warn(\"t is not on the same device as y0. Coercing to y0.device.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss 174.72207641601562 | Likelihood -177.2862548828125 | KL Coef 0.0\n",
      "shape torch.Size([10, 1, 100, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/usr/SSD/intern/taejun/latent_ode/lib/plotting.py:459: UserWarning: The figure layout has changed to tight\n",
      "  self.fig.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 20 | Loss 176.40185546875 | Likelihood -179.26551818847656 | KL Coef 0.09561792499119559\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 30 | Loss 153.34158325195312 | Likelihood -157.91458129882812 | KL Coef 0.18209306240276923\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 40 | Loss 148.54486083984375 | Likelihood -150.39089965820312 | KL Coef 0.2602996266117198\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 50 | Loss 153.61436462402344 | Likelihood -157.6953125 | KL Coef 0.3310282414303197\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 60 | Loss 157.47064208984375 | Likelihood -160.91357421875 | KL Coef 0.39499393286246365\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 70 | Loss 145.32418823242188 | Likelihood -147.4967498779297 | KL Coef 0.4528433576092388\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 80 | Loss 179.29832458496094 | Likelihood -194.10289001464844 | KL Coef 0.505161340399793\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 90 | Loss 156.3390350341797 | Likelihood -159.455810546875 | KL Coef 0.5524767862361895\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 100 | Loss 138.67506408691406 | Likelihood -140.1967315673828 | KL Coef 0.5952680273216762\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 110 | Loss 132.17779541015625 | Likelihood -133.79490661621094 | KL Coef 0.6339676587267709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/usr/SSD/intern/taejun/latent_ode/lib/plotting.py:164: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(figsize=(20, 15), facecolor='white',constrained_layout=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 120 | Loss 129.71328735351562 | Likelihood -131.1417999267578 | KL Coef 0.6689669116789861\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 130 | Loss 129.63851928710938 | Likelihood -131.0623321533203 | KL Coef 0.7006196086876687\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 140 | Loss 127.0517807006836 | Likelihood -128.33204650878906 | KL Coef 0.729245740488006\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 150 | Loss 132.34616088867188 | Likelihood -133.957763671875 | KL Coef 0.7551347009650705\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 160 | Loss 133.24966430664062 | Likelihood -134.862548828125 | KL Coef 0.7785482127611391\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 170 | Loss 132.11297607421875 | Likelihood -133.56341552734375 | KL Coef 0.7997229731425107\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 180 | Loss 126.0180435180664 | Likelihood -127.30708312988281 | KL Coef 0.8188730468740297\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 190 | Loss 128.4090576171875 | Likelihood -131.0646209716797 | KL Coef 0.8361920302919126\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 200 | Loss 120.84419250488281 | Likelihood -122.20075988769531 | KL Coef 0.8518550084524206\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 210 | Loss 117.53180694580078 | Likelihood -119.15422821044922 | KL Coef 0.8660203251420383\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 220 | Loss 119.43822479248047 | Likelihood -120.21990966796875 | KL Coef 0.8788311836429517\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 230 | Loss 134.79071044921875 | Likelihood -136.78143310546875 | KL Coef 0.8904170944366518\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 240 | Loss 116.72835540771484 | Likelihood -120.6347427368164 | KL Coef 0.9008951844811254\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 250 | Loss 156.8253631591797 | Likelihood -169.3037109375 | KL Coef 0.9103713812976754\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 260 | Loss 107.37898254394531 | Likelihood -108.76258850097656 | KL Coef 0.9189414838378187\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 270 | Loss 93.26004791259766 | Likelihood -94.09272003173828 | KL Coef 0.9266921309561118\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 280 | Loss 108.54650115966797 | Likelihood -125.0549087524414 | KL Coef 0.9337016772796147\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 290 | Loss 87.77232360839844 | Likelihood -88.57845306396484 | KL Coef 0.9400409853285345\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 300 | Loss 85.89422607421875 | Likelihood -86.42374420166016 | KL Coef 0.9457741418959368\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 310 | Loss 81.24486541748047 | Likelihood -82.05455780029297 | KL Coef 0.9509591059287142\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 320 | Loss 75.81949615478516 | Likelihood -76.29737091064453 | KL Coef 0.9556482944595236\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 330 | Loss 73.81061553955078 | Likelihood -74.72189331054688 | KL Coef 0.9598891125131245\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 340 | Loss 71.49319458007812 | Likelihood -74.40204620361328 | KL Coef 0.9637244323441748\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 350 | Loss 72.42577362060547 | Likelihood -74.1173324584961 | KL Coef 0.9671930268513026\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 360 | Loss 58.64937973022461 | Likelihood -58.98942947387695 | KL Coef 0.9703299615490228\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 370 | Loss 57.63084411621094 | Likelihood -58.093719482421875 | KL Coef 0.9731669490601144\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 380 | Loss 58.47401809692383 | Likelihood -58.818599700927734 | KL Coef 0.9757326697121692\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 390 | Loss 54.54864501953125 | Likelihood -54.98891067504883 | KL Coef 0.9780530614793677\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 400 | Loss 53.67861557006836 | Likelihood -54.194580078125 | KL Coef 0.9801515822006198\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 410 | Loss 56.65842056274414 | Likelihood -57.46938705444336 | KL Coef 0.9820494467249549\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 420 | Loss 58.010894775390625 | Likelihood -58.65410232543945 | KL Coef 0.9837658413815585\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 430 | Loss 54.00018310546875 | Likelihood -54.64997482299805 | KL Coef 0.9853181179426319\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 440 | Loss 55.2066650390625 | Likelihood -55.98588180541992 | KL Coef 0.9867219690399229\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 450 | Loss 49.54139709472656 | Likelihood -49.86699676513672 | KL Coef 0.9879915868082944\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 460 | Loss 47.9312858581543 | Likelihood -48.18520736694336 | KL Coef 0.9891398063601221\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 470 | Loss 48.120967864990234 | Likelihood -48.440406799316406 | KL Coef 0.9901782355409698\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 480 | Loss 46.087059020996094 | Likelihood -46.23043441772461 | KL Coef 0.9911173722782946\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 490 | Loss 48.80074691772461 | Likelihood -49.60041809082031 | KL Coef 0.9919667107095133\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 500 | Loss 48.443904876708984 | Likelihood -49.34300231933594 | KL Coef 0.9927348371623237\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 510 | Loss 45.16688919067383 | Likelihood -45.29210662841797 | KL Coef 0.9934295169575854\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 520 | Loss 64.30508422851562 | Likelihood -69.74730682373047 | KL Coef 0.9940577729122909\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 530 | Loss 67.63855743408203 | Likelihood -70.22945404052734 | KL Coef 0.9946259563362442\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 540 | Loss 51.513572692871094 | Likelihood -52.78144073486328 | KL Coef 0.9951398112401846\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 550 | Loss 54.30443572998047 | Likelihood -56.40956497192383 | KL Coef 0.9956045324044637\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 560 | Loss 46.33060073852539 | Likelihood -46.80843734741211 | KL Coef 0.9960248178953148\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 570 | Loss 43.25958251953125 | Likelihood -43.55434799194336 | KL Coef 0.996404916559627\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 580 | Loss 41.94858932495117 | Likelihood -42.27716064453125 | KL Coef 0.9967486709783656\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 590 | Loss 42.091224670410156 | Likelihood -42.39474105834961 | KL Coef 0.997059556312878\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 600 | Loss 48.583404541015625 | Likelihood -49.267635345458984 | KL Coef 0.997340715436794\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 610 | Loss 42.615020751953125 | Likelihood -42.987613677978516 | KL Coef 0.997594990708689\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 620 | Loss 39.922340393066406 | Likelihood -40.102108001708984 | KL Coef 0.9978249527067087\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 630 | Loss 41.53695297241211 | Likelihood -42.0582160949707 | KL Coef 0.9980329262156509\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 640 | Loss 56.26899719238281 | Likelihood -58.3238410949707 | KL Coef 0.9982210137292149\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 650 | Loss 42.557308197021484 | Likelihood -43.0869255065918 | KL Coef 0.9983911167050152\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 660 | Loss 48.82942199707031 | Likelihood -52.324378967285156 | KL Coef 0.9985449547872347\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 670 | Loss 39.65632247924805 | Likelihood -39.83518600463867 | KL Coef 0.9986840831912477\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 680 | Loss 38.601810455322266 | Likelihood -38.85737991333008 | KL Coef 0.9988099084259616\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 690 | Loss 38.266788482666016 | Likelihood -38.60429763793945 | KL Coef 0.9989237025128207\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 700 | Loss 41.38917922973633 | Likelihood -42.204830169677734 | KL Coef 0.999026615845218\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 710 | Loss 38.824195861816406 | Likelihood -39.003631591796875 | KL Coef 0.9991196888183176\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 720 | Loss 37.41410446166992 | Likelihood -37.665889739990234 | KL Coef 0.9992038623468565\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 730 | Loss 42.05059051513672 | Likelihood -42.30253982543945 | KL Coef 0.9992799873772575\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 740 | Loss 39.4366340637207 | Likelihood -40.02404022216797 | KL Coef 0.9993488334902116\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 750 | Loss 36.39289474487305 | Likelihood -36.70551300048828 | KL Coef 0.9994110966807014\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 760 | Loss 36.041412353515625 | Likelihood -36.41179656982422 | KL Coef 0.9994674063941131\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 770 | Loss 36.037166595458984 | Likelihood -36.20886993408203 | KL Coef 0.9995183318895716\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 780 | Loss 35.23627471923828 | Likelihood -35.49137878417969 | KL Coef 0.9995643879948252\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 790 | Loss 34.81177520751953 | Likelihood -35.00457763671875 | KL Coef 0.9996060403108612\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 800 | Loss 34.722442626953125 | Likelihood -34.916046142578125 | KL Coef 0.9996437099188669\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 810 | Loss 34.69035339355469 | Likelihood -34.83735275268555 | KL Coef 0.9996777776371197\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 820 | Loss 34.48705291748047 | Likelihood -34.650386810302734 | KL Coef 0.9997085878708442\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 830 | Loss 33.89812088012695 | Likelihood -33.934417724609375 | KL Coef 0.9997364520939512\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 840 | Loss 38.0799674987793 | Likelihood -39.24492263793945 | KL Coef 0.9997616519978635\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 850 | Loss 34.02751541137695 | Likelihood -34.2668342590332 | KL Coef 0.9997844423392536\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 860 | Loss 35.397216796875 | Likelihood -35.900115966796875 | KL Coef 0.9998050535154901\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 870 | Loss 33.421260833740234 | Likelihood -33.685001373291016 | KL Coef 0.9998236938938232\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 880 | Loss 32.91514205932617 | Likelihood -33.10230255126953 | KL Coef 0.9998405519178591\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 890 | Loss 32.443016052246094 | Likelihood -32.58671188354492 | KL Coef 0.9998557980126173\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 900 | Loss 34.794795989990234 | Likelihood -35.42940139770508 | KL Coef 0.9998695863074304\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 910 | Loss 32.94431686401367 | Likelihood -33.26484680175781 | KL Coef 0.9998820561941043\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 920 | Loss 31.972820281982422 | Likelihood -32.086631774902344 | KL Coef 0.9998933337360897\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 930 | Loss 31.730552673339844 | Likelihood -31.923095703125 | KL Coef 0.9999035329429113\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 940 | Loss 31.513355255126953 | Likelihood -31.62240982055664 | KL Coef 0.9999127569227402\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 950 | Loss 31.532527923583984 | Likelihood -31.71502685546875 | KL Coef 0.9999210989247576\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 960 | Loss 31.40953254699707 | Likelihood -31.513330459594727 | KL Coef 0.9999286432818518\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 970 | Loss 31.364093780517578 | Likelihood -31.656661987304688 | KL Coef 0.9999354662631753\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 980 | Loss 31.530616760253906 | Likelihood -31.867984771728516 | KL Coef 0.9999416368451824\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 990 | Loss 30.956567764282227 | Likelihood -31.21601676940918 | KL Coef 0.9999472174089421\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1000 | Loss 33.2867317199707 | Likelihood -33.767757415771484 | KL Coef 0.9999522643707747\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1010 | Loss 31.738893508911133 | Likelihood -31.99824333190918 | KL Coef 0.9999568287525893\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1020 | Loss 95.50837707519531 | Likelihood -104.49446105957031 | KL Coef 0.999960956697686\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1030 | Loss 60.169708251953125 | Likelihood -62.2079963684082 | KL Coef 0.9999646899372381\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1040 | Loss 44.018550872802734 | Likelihood -45.76424026489258 | KL Coef 0.9999680662121707\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1050 | Loss 38.820953369140625 | Likelihood -39.656551361083984 | KL Coef 0.9999711196547001\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1060 | Loss 35.6630859375 | Likelihood -36.21699142456055 | KL Coef 0.9999738811333907\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1070 | Loss 37.684627532958984 | Likelihood -39.084014892578125 | KL Coef 0.9999763785652189\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1080 | Loss 33.938472747802734 | Likelihood -34.792694091796875 | KL Coef 0.999978637197798\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1090 | Loss 32.54621887207031 | Likelihood -32.908382415771484 | KL Coef 0.9999806798646166\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1100 | Loss 31.901365280151367 | Likelihood -32.292667388916016 | KL Coef 0.9999825272158726\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1110 | Loss 81.7275390625 | Likelihood -87.53340148925781 | KL Coef 0.9999841979272346\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1120 | Loss 68.36199188232422 | Likelihood -71.3633041381836 | KL Coef 0.999985708888643\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1130 | Loss 106.00786590576172 | Likelihood -114.30378723144531 | KL Coef 0.9999870753750567\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Epoch 1140 | Loss 1064.4656982421875 | Likelihood -1157.365478515625 | KL Coef 0.9999883112008752\n",
      "shape torch.Size([10, 1, 100, 1])\n",
      "Computing loss... 0\n",
      "shape torch.Size([10, 1, 100, 1])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m kl_coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m itr \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_batches \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.99\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (itr \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_batches \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     37\u001b[0m batch_dict \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_next_batch(data_obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataloader\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 38\u001b[0m train_res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_all_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_traj_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkl_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m train_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/media/usr/SSD/intern/taejun/latent_ode/lib/base_models.py:256\u001b[0m, in \u001b[0;36mVAE_Baseline.compute_all_losses\u001b[0;34m(self, batch_dict, n_traj_samples, kl_coef)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_all_losses\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_dict, n_traj_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, kl_coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m):\n\u001b[1;32m    254\u001b[0m \t\u001b[38;5;66;03m# Condition on subsampled points\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \t\u001b[38;5;66;03m# Make predictions for all the points\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \tpred_y, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reconstruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtp_to_predict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobserved_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobserved_tp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobserved_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_traj_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_traj_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \t\u001b[38;5;66;03m#print(\"get_reconstruction done -- computing likelihood\")\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \tfp_mu, fp_std, fp_enc \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_point\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/media/usr/SSD/intern/taejun/latent_ode/lib/latent_ode.py:60\u001b[0m, in \u001b[0;36mLatentODE.get_reconstruction\u001b[0;34m(self, time_steps_to_predict, truth, truth_time_steps, mask, n_traj_samples, run_backwards, mode)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m \ttruth_w_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((truth, mask), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m first_point_mu, first_point_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_z0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtruth_w_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth_time_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_backwards\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_backwards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m means_z0 \u001b[38;5;241m=\u001b[39m first_point_mu\u001b[38;5;241m.\u001b[39mrepeat(n_traj_samples, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m sigma_z0 \u001b[38;5;241m=\u001b[39m first_point_std\u001b[38;5;241m.\u001b[39mrepeat(n_traj_samples, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/taejun/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/taejun/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/usr/SSD/intern/taejun/latent_ode/lib/encoder_decoder.py:224\u001b[0m, in \u001b[0;36mEncoder_z0_ODE_RNN.forward\u001b[0;34m(self, data, time_steps, run_backwards, save_info)\u001b[0m\n\u001b[1;32m    221\u001b[0m \textra_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m \tlast_yi, last_yi_std, _, extra_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_odernn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_backwards\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_backwards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43msave_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m means_z0 \u001b[38;5;241m=\u001b[39m last_yi\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, n_traj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim)\n\u001b[1;32m    229\u001b[0m std_z0 \u001b[38;5;241m=\u001b[39m last_yi_std\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, n_traj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim)\n",
      "File \u001b[0;32m/media/usr/SSD/intern/taejun/latent_ode/lib/encoder_decoder.py:276\u001b[0m, in \u001b[0;36mEncoder_z0_ODE_RNN.run_odernn\u001b[0;34m(self, data, time_steps, run_backwards, save_info)\u001b[0m\n\u001b[1;32m    273\u001b[0m time_points \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((prev_t, t_i))\n\u001b[1;32m    274\u001b[0m inc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz0_diffeq_solver\u001b[38;5;241m.\u001b[39mode_func(prev_t, prev_y) \u001b[38;5;241m*\u001b[39m (t_i \u001b[38;5;241m-\u001b[39m prev_t)\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(inc)\u001b[38;5;241m.\u001b[39many())\n\u001b[1;32m    278\u001b[0m ode_sol \u001b[38;5;241m=\u001b[39m prev_y \u001b[38;5;241m+\u001b[39m inc\n\u001b[1;32m    279\u001b[0m ode_sol \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((prev_y, ode_sol), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "experimentID = args.load if args.load is not None else int(SystemRandom().random() * 100000)\n",
    "ckpt_path = os.path.join(args.save, f\"experiment_{experimentID}.ckpt\")\n",
    "\n",
    "# 데이터 로딩 및 처리\n",
    "data_obj = data_objects # 'data_objects'는 데이터 로딩 및 전처리를 담당하는 코드 부분에서 정의됨\n",
    "input_dim = data_obj[\"input_dim\"]\n",
    "\n",
    "# 모델 생성\n",
    "obsrv_std = torch.Tensor([0.01]).to(device)\n",
    "z0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))\n",
    "\n",
    "if args.latent_ode:\n",
    "    model = create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device,\n",
    "                                   classif_per_tp=False, # 이 예제에서는 간소화를 위해 False로 설정\n",
    "                                   n_labels=1) # 이 예제에서는 단순화를 위해 1로 설정\n",
    "else:\n",
    "    raise Exception(\"Model not specified\")\n",
    "\n",
    "# # 로그 설정\n",
    "# log_path = f\"logs/{file_name}_{experimentID}.log\"\n",
    "# if not os.path.exists(\"logs/\"):\n",
    "#     os.makedirs(\"logs/\")\n",
    "# logger = utils.get_logger(logpath=log_path, filepath=os.path.abspath(__file__))\n",
    "# logger.info(\" \".join(sys.argv))\n",
    "\n",
    "# 최적화기 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# 훈련 과정\n",
    "num_batches = data_obj[\"n_train_batches\"]\n",
    "for itr in range(1, num_batches * (args.niters + 1)):\n",
    "    optimizer.zero_grad()\n",
    "    utils.update_learning_rate(optimizer, decay_rate=0.999, lowest=args.lr / 10)\n",
    "\n",
    "    kl_coef = 0. if itr // num_batches < 10 else (1-0.99 ** (itr // num_batches - 10))\n",
    "\n",
    "    batch_dict = utils.get_next_batch(data_obj[\"train_dataloader\"])\n",
    "    train_res = model.compute_all_losses(batch_dict, n_traj_samples=10, kl_coef=kl_coef)\n",
    "    train_res[\"loss\"].backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 주기적 로깅\n",
    "    if itr % (10 * num_batches) == 0:\n",
    "        print(f\"Epoch {itr // num_batches} | Loss {train_res['loss'].item()} | Likelihood {train_res['likelihood'].item()} | KL Coef {kl_coef}\")\n",
    "\n",
    "        # 훈련 데이터 시각화 (선택적)\n",
    "        if args.viz:  # 시각화 옵션이 활성화된 경우\n",
    "            plot_name_train = f\"final_17_1/train_finalresult_17_{itr // num_batches:04d}\"\n",
    "            with torch.no_grad():\n",
    "                Visualizations(device).draw_all_plots_one_dim(batch_dict, model.to(device), plot_name=plot_name_train, save=True)\n",
    "            with torch.no_grad():\n",
    "                test_res = compute_loss_all_batches(model, \n",
    "\t\t\t\t\tdata_obj[\"test_dataloader\"], args,\n",
    "\t\t\t\t\tn_batches = data_obj[\"n_test_batches\"],\n",
    "\t\t\t\t\texperimentID = experimentID,\n",
    "\t\t\t\t\tdevice = device,\n",
    "\t\t\t\t\tn_traj_samples = 10, kl_coef = kl_coef)\n",
    "                sample_test = utils.get_next_batch(data_obj[\"test_dataloader\"])\n",
    "                plot_name_test = f\"final_17_1/test_finalresult_17_{itr // num_batches:04d}\"\n",
    "                Visualizations(device).draw_all_plots_one_dim(sample_test ,model.to(device), plot_name=plot_name_test, save = True)\n",
    "\t\t\t\t\n",
    "\n",
    "# 체크포인트 저장 (선택적)\n",
    "# torch.save(model.state_dict(), ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    final_test_res = compute_loss_all_batches(model, \n",
    "                    data_obj[\"test_dataloader\"], args,\n",
    "                    n_batches=data_obj[\"n_test_batches\"],\n",
    "                    experimentID=experimentID,\n",
    "                    device=device,\n",
    "                    n_traj_samples=10, kl_coef=kl_coef)\n",
    "\n",
    "    print(\"Final Test Loss Summary:\")\n",
    "    for key, value in final_test_res.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "# \ttorch.manual_seed(args.random_seed)\n",
    "# \tnp.random.seed(args.random_seed)\n",
    "\n",
    "# \texperimentID = args.load\n",
    "# \tif experimentID is None:\n",
    "# \t\t# Make a new experiment ID\n",
    "# \t\texperimentID = int(SystemRandom().random()*100000)\n",
    "# \tckpt_path = os.path.join(args.save, \"experiment_\" + str(experimentID) + '.ckpt')\n",
    "\n",
    "# \tstart = time.time()\n",
    "# \tprint(\"Sampling dataset of {} training examples\".format(args.n))\n",
    "\t\n",
    "# \tinput_command = sys.argv\n",
    "# \tind = [i for i in range(len(input_command)) if input_command[i] == \"--load\"]\n",
    "# \tif len(ind) == 1:\n",
    "# \t\tind = ind[0]\n",
    "# \t\tinput_command = input_command[:ind] + input_command[(ind+2):]\n",
    "# \tinput_command = \" \".join(input_command)\n",
    "\n",
    "# \tutils.makedirs(\"results/\")\n",
    "\n",
    "# \t##################################################################\n",
    "# \tdata_obj = data_objects\n",
    "# \tinput_dim = data_obj[\"input_dim\"]\n",
    "\n",
    "# \tclassif_per_tp = False\n",
    "# \tif (\"classif_per_tp\" in data_obj):\n",
    "# \t\t# do classification per time point rather than on a time series as a whole\n",
    "# \t\tclassif_per_tp = data_obj[\"classif_per_tp\"]\n",
    "\n",
    "# \tif args.classif and (args.dataset == \"hopper\" or args.dataset == \"periodic\"):\n",
    "# \t\traise Exception(\"Classification task is not available for MuJoCo and 1d datasets\")\n",
    "\n",
    "# \tn_labels = 1\n",
    "# \tif args.classif:\n",
    "# \t\tif (\"n_labels\" in data_obj):\n",
    "# \t\t\tn_labels = data_obj[\"n_labels\"]\n",
    "# \t\telse:\n",
    "# \t\t\traise Exception(\"Please provide number of labels for classification task\")\n",
    "\n",
    "# \t##################################################################\n",
    "# \t# Create the model\n",
    "# \tobsrv_std = 0.01\n",
    "# \tobsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
    "# \tz0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))\n",
    "\n",
    "# \tif args.rnn_vae:\n",
    "# \t\tif args.poisson:\n",
    "# \t\t\tprint(\"Poisson process likelihood not implemented for RNN-VAE: ignoring --poisson\")\n",
    "\n",
    "# \t\t# Create RNN-VAE model\n",
    "# \t\tmodel = RNN_VAE(input_dim, args.latents, \n",
    "# \t\t\tdevice = device, \n",
    "# \t\t\trec_dims = args.rec_dims, \n",
    "# \t\t\tconcat_mask = True, \n",
    "# \t\t\tobsrv_std = obsrv_std,\n",
    "# \t\t\tz0_prior = z0_prior,\n",
    "# \t\t\tuse_binary_classif = args.classif,\n",
    "# \t\t\tclassif_per_tp = classif_per_tp,\n",
    "# \t\t\tlinear_classifier = args.linear_classif,\n",
    "# \t\t\tn_units = args.units,\n",
    "# \t\t\tinput_space_decay = args.input_decay,\n",
    "# \t\t\tcell = args.rnn_cell,\n",
    "# \t\t\tn_labels = n_labels,\n",
    "# \t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "# \t\t\t).to(device)\n",
    "\t\t\n",
    "# \telif args.classic_rnn:\n",
    "# \t\tif args.poisson:\n",
    "# \t\t\tprint(\"Poisson process likelihood not implemented for RNN: ignoring --poisson\")\n",
    "\n",
    "# \t\tif args.extrap:\n",
    "# \t\t\traise Exception(\"Extrapolation for standard RNN not implemented\")\n",
    "# \t\t# Create RNN model\n",
    "# \t\tmodel = Classic_RNN(input_dim, args.latents, device, \n",
    "# \t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
    "# \t\t\tn_units = args.units,\n",
    "# \t\t\tuse_binary_classif = args.classif,\n",
    "# \t\t\tclassif_per_tp = classif_per_tp,\n",
    "# \t\t\tlinear_classifier = args.linear_classif,\n",
    "# \t\t\tinput_space_decay = args.input_decay,\n",
    "# \t\t\tcell = args.rnn_cell,\n",
    "# \t\t\tn_labels = n_labels,\n",
    "# \t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "# \t\t\t).to(device)\n",
    "\t\t\n",
    "# \telif args.ode_rnn:\n",
    "# \t\t# Create ODE-GRU model\n",
    "# \t\tn_ode_gru_dims = args.latents\n",
    "\t\t\t\t\n",
    "# \t\tif args.poisson:\n",
    "# \t\t\tprint(\"Poisson process likelihood not implemented for ODE-RNN: ignoring --poisson\")\n",
    "\n",
    "# \t\tif args.extrap:\n",
    "# \t\t\traise Exception(\"Extrapolation for ODE-RNN not implemented\")\n",
    "\n",
    "# \t\tode_func_net = utils.create_net(n_ode_gru_dims, n_ode_gru_dims, \n",
    "# \t\t\tn_layers = args.rec_layers, n_units = args.units, nonlinear = nn.Tanh)\n",
    "\n",
    "# \t\trec_ode_func = ODEFunc(\n",
    "# \t\t\tinput_dim = input_dim, \n",
    "# \t\t\tlatent_dim = n_ode_gru_dims,\n",
    "# \t\t\tode_func_net = ode_func_net,\n",
    "# \t\t\tdevice = device).to(device)\n",
    "\n",
    "# \t\tz0_diffeq_solver = DiffeqSolver(input_dim, rec_ode_func, \"euler\", args.latents, \n",
    "# \t\t\todeint_rtol = 1e-3, odeint_atol = 1e-4, device = device)\n",
    "\t\n",
    "# \t\tmodel = ODE_RNN(input_dim, n_ode_gru_dims, device = device, \n",
    "# \t\t\tz0_diffeq_solver = z0_diffeq_solver, n_gru_units = args.gru_units,\n",
    "# \t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
    "# \t\t\tuse_binary_classif = args.classif,\n",
    "# \t\t\tclassif_per_tp = classif_per_tp,\n",
    "# \t\t\tn_labels = n_labels,\n",
    "# \t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "# \t\t\t).to(device)\n",
    "# \telif args.latent_ode:\n",
    "# \t\tmodel = create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device, \n",
    "# \t\t\tclassif_per_tp = classif_per_tp,\n",
    "# \t\t\tn_labels = n_labels)\n",
    "# \telse:\n",
    "# \t\traise Exception(\"Model not specified\")\n",
    "\n",
    "# \t##################################################################\n",
    "\n",
    "# \tif args.viz:\n",
    "# \t\tviz = Visualizations(device)\n",
    "\n",
    "# \t##################################################################\n",
    "\t\n",
    "# \t#Load checkpoint and evaluate the model\n",
    "# \tif args.load is not None:\n",
    "# \t\tutils.get_ckpt_model(ckpt_path, model, device)\n",
    "# \t\texit()\n",
    "\n",
    "# \t##################################################################\n",
    "# \t# Training\n",
    "\n",
    "# \tlog_path = \"logs/\" + file_name + \"_\" + str(experimentID) + \".log\"\n",
    "# \tif not os.path.exists(\"logs/\"):\n",
    "# \t\tutils.makedirs(\"logs/\")\n",
    "# \tlogger = utils.get_logger(logpath=log_path, filepath=os.path.abspath(file_name))\n",
    "# \tlogger.info(input_command)\n",
    "\n",
    "# \toptimizer = optim.Adamax(model.parameters(), lr=args.lr)\n",
    "\n",
    "# \tnum_batches = data_obj[\"n_train_batches\"]\n",
    "\n",
    "# \tfor itr in range(1, num_batches * (args.niters + 1)):\n",
    "# \t\toptimizer.zero_grad()\n",
    "# \t\tutils.update_learning_rate(optimizer, decay_rate=0.999, lowest=args.lr / 10)\n",
    "\n",
    "# \t\twait_until_kl_inc = 10\n",
    "# \t\tif itr // num_batches < wait_until_kl_inc:\n",
    "# \t\t\tkl_coef = 0.\n",
    "# \t\telse:\n",
    "# \t\t\tkl_coef = (1-0.99**(itr // num_batches - wait_until_kl_inc))\n",
    "\n",
    "# \t\tbatch_dict = utils.get_next_batch(data_obj[\"train_dataloader\"])\n",
    "# \t\tprint(batch_dict['observed_data'])\n",
    "# \t\ttrain_res = model.compute_all_losses(batch_dict, n_traj_samples=10, kl_coef=kl_coef)\n",
    "# \t\ttrain_res[\"loss\"].backward()\n",
    "# \t\toptimizer.step()\n",
    "\n",
    "# \t\t# 주기적으로 로깅 및 시각화를 위한 조건문\n",
    "# \t\tif itr % (10 * num_batches) == 0:  # 예를 들어, 10 epoch 마다 로그와 시각화 수행\n",
    "# \t\t\tlogger.info(f\"Epoch {itr // num_batches} | Loss {train_res['loss'].item()} | Likelihood {train_res['likelihood'].item()} | KL Coef {kl_coef}\")\n",
    "\n",
    "# \t\t\t# 훈련 데이터 시각화\n",
    "# \t\t\tplot_name_train = f\"train_result_{itr // num_batches:04d}\"\n",
    "# \t\t\twith torch.no_grad():  # 시각화 동안에는 기울기 계산을 하지 않음\n",
    "# \t\t\t\tVisualizations(device).draw_all_plots_one_dim(batch_dict, model.to(device), plot_name=plot_name_train, save=True)\n",
    "\n",
    "# \t\t\t# 추가 정보 로깅\n",
    "# \t\t\tlogger.info(f\"Train loss (one batch): {train_res['loss'].item()}\")\n",
    "# \t\t\tif 'ce_loss' in train_res:\n",
    "# \t\t\t\tlogger.info(f\"Train CE loss (one batch): {train_res['ce_loss'].item()}\")\n",
    "\n",
    "\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\t\t\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 523, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dict['observed_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object inf_generator at 0x7fcfd36ec190>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_obj[\"test_dataloader\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taejun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
