{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import SystemRandom\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "import torch.optim as optim\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.plotting import *\n",
    "\n",
    "from lib.rnn_baselines import *\n",
    "from lib.ode_rnn import *\n",
    "from lib.create_latent_ode_model import create_LatentODE_model\n",
    "from lib.parse_datasets import parse_datasets\n",
    "from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
    "from lib.diffeq_solver import DiffeqSolver\n",
    "from mujoco_physics import HopperPhysics\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.diffeq_solver import DiffeqSolver\n",
    "from generate_timeseries import Periodic_1d\n",
    "from torch.distributions import uniform\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from mujoco_physics import HopperPhysics\n",
    "from physionet import variable_time_collate_fn, get_data_min_max\n",
    "from person_activity import PersonActivity, variable_time_collate_fn_activity\n",
    "\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from lib.utils import compute_loss_all_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Number of iterations: 30000\n",
      "Learning rate: 0.01\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Generative model for noisy data based on ODE\n",
    "parser = argparse.ArgumentParser('Latent ODE')\n",
    "parser.add_argument('-n',  type=int, default=10000, help=\"Size of the dataset\")\n",
    "parser.add_argument('--niters', type=int, default=20)\n",
    "parser.add_argument('--lr',  type=float, default=1e-2, help=\"Starting learning rate.\")\n",
    "parser.add_argument('-b', '--batch-size', type=int, default=256)\n",
    "parser.add_argument('--viz', action='store_true', default=False, help=\"Show plots while training\")\n",
    "\n",
    "parser.add_argument('--save', type=str, default='experiments/', help=\"Path for save checkpoints\")\n",
    "parser.add_argument('--load', type=str, default=None, help=\"ID of the experiment to load for evaluation. If None, run a new experiment.\")\n",
    "parser.add_argument('-r', '--random-seed', type=int, default=1991, help=\"Random_seed\")\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default='physionet', help=\"Dataset to load. Available: physionet, activity, hopper, periodic\")\n",
    "parser.add_argument('-s', '--sample-tp', type=float, default=None, help=\"Number of time points to sub-sample.\"\n",
    "\t\"If > 1, subsample exact number of points. If the number is in [0,1], take a percentage of available points per time series. If None, do not subsample\")\n",
    "\n",
    "parser.add_argument('-c', '--cut-tp', type=int, default=None, help=\"Cut out the section of the timeline of the specified length (in number of points).\"\n",
    "\t\"Used for periodic function demo.\")\n",
    "\n",
    "parser.add_argument('--quantization', type=float, default=5, help=\"Quantization on the physionet dataset.\"\n",
    "\t\"Value 1 means quantization by 1 hour, value 0.1 means quantization by 0.1 hour = 6 min\")\n",
    "\n",
    "parser.add_argument('--latent-ode', action='store_true', default=True,  help=\"Run Latent ODE seq2seq model\")\n",
    "parser.add_argument('--z0-encoder', type=str, default='odernn', help=\"Type of encoder for Latent ODE model: odernn or rnn\")\n",
    "\n",
    "parser.add_argument('--classic-rnn', action='store_true', help=\"Run RNN baseline: classic RNN that sees true points at every point. Used for interpolation only.\")\n",
    "parser.add_argument('--rnn-cell', default=\"gru\", help=\"RNN Cell type. Available: gru (default), expdecay\")\n",
    "parser.add_argument('--input-decay', action='store_true', help=\"For RNN: use the input that is the weighted average of impirical mean and previous value (like in GRU-D)\")\n",
    "\n",
    "parser.add_argument('--ode-rnn', action='store_true', help=\"Run ODE-RNN baseline: RNN-style that sees true points at every point. Used for interpolation only.\")\n",
    "\n",
    "parser.add_argument('--rnn-vae', action='store_true', help=\"Run RNN baseline: seq2seq model with sampling of the h0 and ELBO loss.\")\n",
    "\n",
    "parser.add_argument('-l', '--latents', type=int, default=6, help=\"Size of the latent state\")\n",
    "parser.add_argument('--rec-dims', type=int, default=40, help=\"Dimensionality of the recognition model (ODE or RNN).\")\n",
    "\n",
    "parser.add_argument('--rec-layers', type=int, default=3, help=\"Number of layers in ODE func in recognition ODE\")\n",
    "parser.add_argument('--gen-layers', type=int, default=3, help=\"Number of layers in ODE func in generative ODE\")\n",
    "\n",
    "parser.add_argument('-u', '--units', type=int, default=50, help=\"Number of units per layer in ODE func\")\n",
    "parser.add_argument('-g', '--gru-units', type=int, default=100, help=\"Number of units per layer in each of GRU update networks\")\n",
    "\n",
    "parser.add_argument('--poisson', action='store_true', help=\"Model poisson-process likelihood for the density of events in addition to reconstruction.\")\n",
    "parser.add_argument('--classif', action='store_true', help=\"Include binary classification loss -- used for Physionet dataset for hospiral mortality\")\n",
    "\n",
    "parser.add_argument('--linear-classif', action='store_true', help=\"If using a classifier, use a linear classifier instead of 1-layer NN\")\n",
    "parser.add_argument('--extrap', action='store_true', help=\"Set extrapolation mode. If this flag is not set, run interpolation mode.\")\n",
    "\n",
    "parser.add_argument('-t', '--timepoints', type=int, default=100, help=\"Total number of time-points\")\n",
    "parser.add_argument('--max-t',  type=float, default=5., help=\"We subsample points in the interval [0, args.max_tp]\")\n",
    "parser.add_argument('--noise-weight', type=float, default=0.01, help=\"Noise amplitude for generated traejctories\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.n = 10000\n",
    "        self.niters = 1000\n",
    "        self.lr = 1e-2\n",
    "        self.batch_size = 128\n",
    "        self.viz = True\n",
    "        self.save = 'experiments/'\n",
    "        self.load = None\n",
    "        self.random_seed = 1991\n",
    "        self.dataset = 'physionet'\n",
    "        self.sample_tp = 0.6\n",
    "        self.cut_tp = None\n",
    "        self.quantization = 5\n",
    "        self.latent_ode = True\n",
    "        self.z0_encoder = 'odernn'\n",
    "        self.classic_rnn = False\n",
    "        self.rnn_cell = \"gru\"\n",
    "        self.input_decay = False\n",
    "        self.ode_rnn = False\n",
    "        self.rnn_vae = False\n",
    "        self.latents = 20\n",
    "        self.rec_dims = 10\n",
    "        self.rec_layers = 5\n",
    "        self.gen_layers = 5\n",
    "        self.units = 50\n",
    "        self.gru_units = 100\n",
    "        self.poisson = False\n",
    "        self.classif = False\n",
    "        self.linear_classif = False\n",
    "        self.extrap = False\n",
    "        self.timepoints = 100\n",
    "        self.max_t = 5.\n",
    "        self.noise_weight = 0.01\n",
    "\n",
    "# args 객체를 생성하고 필요한 설정을 할당합니다\n",
    "args = Args()\n",
    "\n",
    "# 몇 가지 설정을 수정합니다\n",
    "args.batch_size = 64\n",
    "args.classif = False\n",
    "args.quantization = 5\n",
    "args.niters = 30000\n",
    "args.n = 1000\n",
    "args.sample_tp = None\n",
    "args.latents = 20\n",
    "args.rec_dims = 10\n",
    "args.rec_layers = 5\n",
    "args.gen_layers = 5\n",
    "args.latent_ode = True\n",
    "args.viz = True\n",
    "args.max_t= 3000\n",
    "\n",
    "\n",
    "# 사용할 디바이스를 설정합니다\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 파일 이름을 설정합니다\n",
    "file_name = 'taejun_sim_'\n",
    "\n",
    "# 설정을 확인합니다\n",
    "print(\"Batch size:\", args.batch_size)\n",
    "print(\"Number of iterations:\", args.niters)\n",
    "print(\"Learning rate:\", args.lr)\n",
    "print(\"Device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Args at 0x7f0b6acb5c40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from jdcal import jd2gcal\n",
    "from datetime import datetime\n",
    "\n",
    "class CustomClass(object):\n",
    "    params = ['Magnitude']  # Uncertainty_of_Magnitude 제외\n",
    "\n",
    "    params_dict = {k: i for i, k in enumerate(params)}\n",
    "\n",
    "    def __init__(self, root, train=True, preprocess=False,\n",
    "                 quantization=1, n_samples=None, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.quantization = quantization\n",
    "        self.device = device\n",
    "        self.reduce = \"average\"\n",
    "        if preprocess:\n",
    "            self.preprocess()\n",
    "\n",
    "        self.data = torch.load(os.path.join(self.root, 'lc_' + str(self.quantization) + '.pt'))\n",
    "        self.labels = torch.zeros(len(self.data))\n",
    "\n",
    "        # if n_samples is not None:\n",
    "        #     self.data = self.data[:n_samples]\n",
    "        #     self.labels = self.labels[:n_samples]\n",
    "\n",
    "    def preprocess(self):\n",
    "        simulation_data_root = self.root\n",
    "        data_list = [f for f in os.listdir(simulation_data_root) if f.endswith('.lc')]\n",
    "        data_list.sort()\n",
    "        \n",
    "        # Process only up to 1000 files\n",
    "        data_list = data_list[:1000]\n",
    "        \n",
    "        light_curves = []\n",
    "\n",
    "        for name in tqdm(data_list):\n",
    "            lc_name = name.partition('.')[0]\n",
    "            file_path = os.path.join(simulation_data_root, name)\n",
    "            \n",
    "            # Step 1: Compute max and min values\n",
    "            with open(file_path) as f:\n",
    "                next(f)  # Skip header\n",
    "                magnitudes = [float(line.rstrip().split(' ')[1]) for line in f]\n",
    "                max_magnitude = max(magnitudes)\n",
    "                min_magnitude = min(magnitudes)\n",
    "            \n",
    "            # Step 2: Process the file again to normalize and store data\n",
    "            with open(file_path) as f:\n",
    "                next(f)  # Skip header again\n",
    "                lines = f.readlines()\n",
    "                prev_time = 0\n",
    "                tt = [0.]\n",
    "                vals = [torch.zeros(len(self.params), device=self.device)]\n",
    "                mask = [torch.zeros(len(self.params), device=self.device)]\n",
    "                for line in lines:\n",
    "                    time, magnitude = line.rstrip().split(' ')[:2]\n",
    "                    time = float(time)\n",
    "                    magnitude = float(magnitude)\n",
    "\n",
    "                    # Normalize the magnitude\n",
    "                    normalized_magnitude = (magnitude - min_magnitude) / (max_magnitude - min_magnitude)\n",
    "\n",
    "                    if time != prev_time:\n",
    "                        tt.append(time)\n",
    "                        vals.append(torch.zeros(len(self.params), device=self.device))\n",
    "                        mask.append(torch.zeros(len(self.params), device=self.device))\n",
    "                        prev_time = time\n",
    "\n",
    "                    vals[-1][0] = normalized_magnitude  # Use normalized value\n",
    "                    mask[-1][0] = 1\n",
    "\n",
    "            tt = torch.tensor(tt, device=self.device)[1:]\n",
    "            vals = torch.stack(vals)[1:]\n",
    "            mask = torch.stack(mask)[1:]\n",
    "            labels = None\n",
    "            light_curves.append((lc_name, tt, vals, mask, labels))\n",
    "\n",
    "        torch.save(light_curves, os.path.join(self.root, 'lc_' + str(self.quantization) + '.pt'))\n",
    "        print('Done!')\n",
    "\n",
    "\n",
    "    # def preprocess(self):\n",
    "    #     simulation_data_root = self.root \n",
    "    #     data_list = [f for f in os.listdir(simulation_data_root) if f.endswith('.txt')]\n",
    "    #     data_list.sort()\n",
    "    #     light_curves = []\n",
    "\n",
    "    #     # 전체 데이터셋에 대한 최소값과 최대값을 초기화\n",
    "    #     global_min = float('inf')\n",
    "    #     global_max = float('-inf')\n",
    "\n",
    "    #     # 최소값과 최대값 찾기\n",
    "    #     for name in tqdm(data_list):\n",
    "    #         with open(os.path.join(simulation_data_root, name)) as f:\n",
    "    #             next(f)  # 헤더 건너뛰기\n",
    "    #             for line in f:\n",
    "    #                 _, magnitude = line.rstrip().split(' ')[:2]\n",
    "    #                 magnitude = float(magnitude)\n",
    "    #                 global_min = min(global_min, magnitude)\n",
    "    #                 global_max = max(global_max, magnitude)\n",
    "\n",
    "    #     # 데이터 스케일링 및 처리\n",
    "    #     for name in tqdm(data_list):\n",
    "    #         lc_name = name.partition('.')[0]\n",
    "    #         with open(os.path.join(simulation_data_root, name)) as f:\n",
    "    #             next(f)\n",
    "    #             lines = f.readlines()\n",
    "    #             prev_time = 0\n",
    "    #             tt = [0.]\n",
    "    #             vals = [torch.zeros(len(self.params), device=self.device)]\n",
    "    #             mask = [torch.zeros(len(self.params), device=self.device)]\n",
    "    #             for line in lines:\n",
    "    #                 time, magnitude = line.rstrip().split(' ')[:2]\n",
    "    #                 time = float(time)\n",
    "    #                 magnitude = float(magnitude)\n",
    "\n",
    "    #                 # 스케일링: (magnitude - global_min) / (global_max - global_min)\n",
    "    #                 scaled_magnitude = (magnitude - global_min) / (global_max - global_min)\n",
    "\n",
    "    #                 if time != prev_time:\n",
    "    #                     tt.append(time)\n",
    "    #                     vals.append(torch.zeros(len(self.params), device=self.device))\n",
    "    #                     mask.append(torch.zeros(len(self.params), device=self.device))\n",
    "    #                     prev_time = time\n",
    "\n",
    "    #                 vals[-1][0] = scaled_magnitude  # 스케일된 값 사용\n",
    "    #                 mask[-1][0] = 1\n",
    "\n",
    "    #         tt = torch.tensor(tt, device=self.device)[1:]\n",
    "    #         vals = torch.stack(vals)[1:]\n",
    "    #         mask = torch.stack(mask)[1:]\n",
    "    #         labels = None\n",
    "    #         light_curves.append((lc_name, tt, vals, mask, labels))\n",
    "\n",
    "    #     torch.save(light_curves, os.path.join(self.root, 'lc_' + str(self.quantization) + '.pt'))\n",
    "    #     print('Done!')\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        fmt_str += '    Split: {}\\n'.format('train' if self.train else 'test')\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        fmt_str += '    Quantization: {}\\n'.format(self.quantization)\n",
    "        fmt_str += '    Reduce: {}\\n'.format(self.reduce)\n",
    "        return fmt_str\n",
    "\n",
    "    def visualize(self, timesteps, data, mask, plot_name):\n",
    "        width = 15\n",
    "        height = 15\n",
    "        timesteps.to\n",
    "        non_zero_attributes = (torch.sum(mask,0) > 2).numpy()\n",
    "        non_zero_idx = [i for i in range(len(non_zero_attributes)) if non_zero_attributes[i] == 1.]\n",
    "        n_non_zero = sum(non_zero_attributes)\n",
    "\n",
    "        mask = mask[:, non_zero_idx]\n",
    "        data = data[:, non_zero_idx]\n",
    "\n",
    "        params_non_zero = [self.params[i] for i in non_zero_idx]\n",
    "        params_dict = {k: i for i, k in enumerate(params_non_zero)}\n",
    "\n",
    "        n_col = 3\n",
    "        n_row = n_non_zero // n_col + (n_non_zero % n_col > 0)\n",
    "        fig, ax_list = plt.subplots(n_row, n_col, figsize=(width, height), facecolor='white')\n",
    "\n",
    "        #for i in range(len(self.params)):\n",
    "        for i in range(n_non_zero):\n",
    "            param = params_non_zero[i]\n",
    "            param_id = params_dict[param]\n",
    "\n",
    "            tp_mask = mask[:,param_id].long()\n",
    "\n",
    "            tp_cur_param = timesteps[tp_mask == 1.]\n",
    "            data_cur_param = data[tp_mask == 1., param_id]\n",
    "\n",
    "            ax_list[i // n_col, i % n_col].plot(tp_cur_param.numpy(), data_cur_param.numpy(),  marker='o') \n",
    "            ax_list[i // n_col, i % n_col].set_title(param)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(plot_name)\n",
    "        plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드에 구현된거 하지만 잘 안된다 수정해서 해보자\n",
    "def variable_time_collate_fn(batch, args, device = device, data_type = \"train\", \n",
    "\tdata_min = None, data_max = None):\n",
    "\tD = batch[0][2].shape[1]\n",
    "\tcombined_tt, inverse_indices = torch.unique(torch.cat([ex[1] for ex in batch]), sorted=True, return_inverse=True)\n",
    "\tcombined_tt = combined_tt.to(device)\n",
    "\n",
    "\toffset = 0\n",
    "\tcombined_vals = torch.zeros([len(batch), len(combined_tt), D]).to(device)\n",
    "\tcombined_mask = torch.zeros([len(batch), len(combined_tt), D]).to(device)\n",
    "\t\n",
    "\tcombined_labels = None\n",
    "\tN_labels = 1\n",
    "\n",
    "\tcombined_labels = torch.zeros(len(batch), N_labels) + torch.tensor(float('nan'))\n",
    "\tcombined_labels = combined_labels.to(device = device)\n",
    "\t\n",
    "\tfor b, (record_id, tt, vals, mask, labels) in enumerate(batch):\n",
    "\t\ttt = tt.to(device)\n",
    "\t\tvals = vals.to(device)\n",
    "\t\tmask = mask.to(device)\n",
    "\t\tif labels is not None:\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\tindices = inverse_indices[offset:offset + len(tt)]\n",
    "\t\toffset += len(tt)\n",
    "\n",
    "\t\tcombined_vals[b, indices] = vals\n",
    "\t\tcombined_mask[b, indices] = mask\n",
    "\n",
    "\t\tif labels is not None:\n",
    "\t\t\tcombined_labels[b] = labels\n",
    "\n",
    "\tcombined_vals, _, _ = utils.normalize_masked_data(combined_vals, combined_mask, \n",
    "\t \tatt_min = data_min, att_max = data_max)\n",
    "\n",
    "\tif torch.max(combined_tt) != 0.:\n",
    "\t\tcombined_tt = combined_tt / torch.max(combined_tt)\n",
    "\t\n",
    "\tdata_dict = {\n",
    "\t\t\"data\": combined_vals, \n",
    "\t\t\"time_steps\": combined_tt,\n",
    "\t\t\"mask\": combined_mask,\n",
    "\t\t\"labels\": combined_labels}\n",
    "\t\n",
    "\tdata_dict = utils.split_and_subsample_batch(data_dict, args, data_type = data_type)\n",
    "\treturn data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset_obj = CustomClass(root='/home/intern/SSD/intern/taejun/r_light_curves', train=True, preprocess=True,quantization=5,n_samples =  args.n, device=device)\n",
    "\t\t# Use custom collate_fn to combine samples with arbitrary time observations.\n",
    "\t\t# Returns the dataset along with mask and time steps\n",
    "\n",
    "\n",
    "# Combine and shuffle samples from physionet Train and physionet Test\n",
    "total_dataset = train_dataset_obj[:len(train_dataset_obj)]\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle and split\n",
    "train_data, test_data = model_selection.train_test_split(total_dataset, train_size= 0.8, \n",
    "    random_state = 42, shuffle = True)\n",
    "\n",
    "record_id, tt, vals, mask, labels = train_data[0]\n",
    "\n",
    "n_samples = len(total_dataset)\n",
    "input_dim = vals.size(-1)\n",
    "\n",
    "batch_size = min(min(len(train_dataset_obj), args.batch_size), args.n)\n",
    "data_min, data_max = get_data_min_max(total_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size= batch_size, shuffle=False, \n",
    "    collate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"train\",\n",
    "        data_min = data_min, data_max = data_max))\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size, shuffle=False, \n",
    "    collate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"train\",\n",
    "        data_min = data_min, data_max = data_max))\n",
    "\n",
    "attr_names = train_dataset_obj.params\n",
    "data_objects = {\"dataset_obj\": train_dataset_obj, \n",
    "            \"train_dataloader\": utils.inf_generator(train_dataloader), \n",
    "            \"test_dataloader\": utils.inf_generator(test_dataloader),\n",
    "            \"input_dim\": input_dim,\n",
    "            \"n_train_batches\": len(train_dataloader),\n",
    "            \"n_test_batches\": len(test_dataloader),\n",
    "            \"attr\": attr_names, #optional\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.sample_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt_file_path = '/home/intern/SSD/intern/taejun/data_normal/lc_5.pt'\n",
    "\n",
    "# # .pt 파일 로드\n",
    "# light_curves_data = torch.load(pt_file_path)\n",
    "\n",
    "# # 로드된 데이터의 타입과 크기 확인\n",
    "# print(f\"Loaded data type: {type(light_curves_data)}\")\n",
    "# print(f\"Number of light curves in the dataset: {len(light_curves_data)}\")\n",
    "\n",
    "# # 첫 번째 광도곡선 데이터의 구조 확인\n",
    "# first_light_curve = light_curves_data[0]\n",
    "# print(f\"Structure of a single light curve data: {type(first_light_curve)}\")\n",
    "# print(f\"Record ID: {first_light_curve[0]}\")\n",
    "# print(f\"Time stamps tensor shape: {first_light_curve[1].shape}\")\n",
    "# print(f\"Magnitude tensor shape: {first_light_curve[2].shape}\")\n",
    "# print(f\"Mask tensor shape: {first_light_curve[3].shape}\")\n",
    "# #print(f\"Labels tensor shape: {first_light_curve[4].shape}\")\n",
    "\n",
    "# # 첫 번째 광도곡선 데이터의 일부 내용 출력\n",
    "# print(\"\\nSample data from the first light curve:\")\n",
    "# print(f\"Time stamps: {first_light_curve[1]}\")  # 처음 5개의 시간 스탬프\n",
    "# print(f\"Magnitude: {first_light_curve[2]}\")  # 처음 5개의 광도 값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader:\n",
      "Batch 1:\n",
      "observed_data.shape: torch.Size([64, 199, 1])\n",
      "data_to_predict: torch.Size([64, 199, 1])\n",
      "Batch 2:\n",
      "observed_data.shape: torch.Size([64, 200, 1])\n",
      "data_to_predict: torch.Size([64, 200, 1])\n",
      "Batch 3:\n",
      "observed_data.shape: torch.Size([64, 200, 1])\n",
      "data_to_predict: torch.Size([64, 200, 1])\n",
      "Batch 4:\n",
      "observed_data.shape: torch.Size([64, 193, 1])\n",
      "data_to_predict: torch.Size([64, 193, 1])\n",
      "Batch 5:\n",
      "observed_data.shape: torch.Size([64, 199, 1])\n",
      "data_to_predict: torch.Size([64, 199, 1])\n",
      "Batch 6:\n",
      "observed_data.shape: torch.Size([64, 201, 1])\n",
      "data_to_predict: torch.Size([64, 201, 1])\n",
      "Batch 7:\n",
      "observed_data.shape: torch.Size([64, 205, 1])\n",
      "data_to_predict: torch.Size([64, 205, 1])\n",
      "Batch 8:\n",
      "observed_data.shape: torch.Size([64, 191, 1])\n",
      "data_to_predict: torch.Size([64, 191, 1])\n",
      "Batch 9:\n",
      "observed_data.shape: torch.Size([64, 195, 1])\n",
      "data_to_predict: torch.Size([64, 195, 1])\n",
      "Batch 10:\n",
      "observed_data.shape: torch.Size([64, 204, 1])\n",
      "data_to_predict: torch.Size([64, 204, 1])\n",
      "Batch 11:\n",
      "observed_data.shape: torch.Size([64, 191, 1])\n",
      "data_to_predict: torch.Size([64, 191, 1])\n",
      "Batch 12:\n",
      "observed_data.shape: torch.Size([64, 206, 1])\n",
      "data_to_predict: torch.Size([64, 206, 1])\n",
      "Batch 13:\n",
      "observed_data.shape: torch.Size([32, 186, 1])\n",
      "data_to_predict: torch.Size([32, 186, 1])\n"
     ]
    }
   ],
   "source": [
    "# `train_dataloader`에서 첫 번째 배치를 가져와서 형태와 내용을 확인합니다.\n",
    "print(\"Train DataLoader:\")\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(\"observed_data.shape:\", batch['observed_data'].shape)\n",
    "    #print(\"observed_data:\", batch['observed_data'])\n",
    "    print(\"data_to_predict:\", batch['data_to_predict'].shape)\n",
    "    #여기에서 필요한 다른 키들도 확인할 수 있습니다.\n",
    "    # 예를\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory where you want to save the file\n",
    "directory = \"experiments\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss 211.82325744628906 | Likelihood -215.18516540527344 | KL Coef 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/usr/SSD/intern/taejun/latent_ode/lib/plotting.py:164: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(figsize=(20, 15), facecolor='white',constrained_layout=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 20 | Loss 123.16305541992188 | Likelihood -125.56639862060547 | KL Coef 0.09561792499119559\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 30 | Loss 112.76026153564453 | Likelihood -115.2221908569336 | KL Coef 0.18209306240276923\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 40 | Loss 121.59784698486328 | Likelihood -123.54615783691406 | KL Coef 0.2602996266117198\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 50 | Loss 106.22374725341797 | Likelihood -108.4842758178711 | KL Coef 0.3310282414303197\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 60 | Loss 104.63167572021484 | Likelihood -107.0267562866211 | KL Coef 0.39499393286246365\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 70 | Loss 91.68841552734375 | Likelihood -93.28572082519531 | KL Coef 0.4528433576092388\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 80 | Loss 85.97193908691406 | Likelihood -87.57601928710938 | KL Coef 0.505161340399793\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 90 | Loss 89.5466079711914 | Likelihood -91.35411834716797 | KL Coef 0.5524767862361895\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 100 | Loss 87.60295867919922 | Likelihood -89.60945892333984 | KL Coef 0.5952680273216762\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 110 | Loss 87.11930847167969 | Likelihood -90.45051574707031 | KL Coef 0.6339676587267709\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 120 | Loss 83.35911560058594 | Likelihood -84.50569915771484 | KL Coef 0.6689669116789861\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 130 | Loss 84.54541015625 | Likelihood -86.98844146728516 | KL Coef 0.7006196086876687\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 140 | Loss 79.49263000488281 | Likelihood -80.58153533935547 | KL Coef 0.729245740488006\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 150 | Loss 78.76216888427734 | Likelihood -82.13817596435547 | KL Coef 0.7551347009650705\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 160 | Loss 93.87059783935547 | Likelihood -95.62593078613281 | KL Coef 0.7785482127611391\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 170 | Loss 89.35151672363281 | Likelihood -93.91471862792969 | KL Coef 0.7997229731425107\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 180 | Loss 82.3136215209961 | Likelihood -84.03350067138672 | KL Coef 0.8188730468740297\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 190 | Loss 145.99044799804688 | Likelihood -148.5367889404297 | KL Coef 0.8361920302919126\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 200 | Loss 114.86225128173828 | Likelihood -117.63249969482422 | KL Coef 0.8518550084524206\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 210 | Loss 106.79841613769531 | Likelihood -108.91987609863281 | KL Coef 0.8660203251420383\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 220 | Loss 106.04698181152344 | Likelihood -108.4479751586914 | KL Coef 0.8788311836429517\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 230 | Loss 96.55968475341797 | Likelihood -97.13155364990234 | KL Coef 0.8904170944366518\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 240 | Loss 102.37823486328125 | Likelihood -106.2216567993164 | KL Coef 0.9008951844811254\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 250 | Loss 88.82505798339844 | Likelihood -89.55817413330078 | KL Coef 0.9103713812976754\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 260 | Loss 157.82350158691406 | Likelihood -161.7091522216797 | KL Coef 0.9189414838378187\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 270 | Loss 105.44922637939453 | Likelihood -106.56465148925781 | KL Coef 0.9266921309561118\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 280 | Loss 101.93632507324219 | Likelihood -104.7171859741211 | KL Coef 0.9337016772796147\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 290 | Loss 89.47288513183594 | Likelihood -90.04830169677734 | KL Coef 0.9400409853285345\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 300 | Loss 89.706787109375 | Likelihood -89.94750213623047 | KL Coef 0.9457741418959368\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 310 | Loss 85.85589599609375 | Likelihood -86.2186508178711 | KL Coef 0.9509591059287142\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 320 | Loss 84.73261260986328 | Likelihood -85.09703826904297 | KL Coef 0.9556482944595236\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 330 | Loss 83.37067413330078 | Likelihood -83.6936264038086 | KL Coef 0.9598891125131245\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 340 | Loss 81.06945037841797 | Likelihood -81.1371078491211 | KL Coef 0.9637244323441748\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 350 | Loss 79.79110717773438 | Likelihood -80.0314712524414 | KL Coef 0.9671930268513026\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 360 | Loss 138.6823272705078 | Likelihood -148.48370361328125 | KL Coef 0.9703299615490228\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 370 | Loss 329.4708251953125 | Likelihood -346.36358642578125 | KL Coef 0.9731669490601144\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 380 | Loss 288.9889221191406 | Likelihood -294.4225769042969 | KL Coef 0.9757326697121692\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 390 | Loss 257.9804382324219 | Likelihood -258.464599609375 | KL Coef 0.9780530614793677\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 400 | Loss 236.62350463867188 | Likelihood -240.5204620361328 | KL Coef 0.9801515822006198\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 410 | Loss 221.91424560546875 | Likelihood -227.15782165527344 | KL Coef 0.9820494467249549\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 420 | Loss 214.2969512939453 | Likelihood -218.82530212402344 | KL Coef 0.9837658413815585\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 430 | Loss 222.2895965576172 | Likelihood -223.3204803466797 | KL Coef 0.9853181179426319\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 440 | Loss 308.2916259765625 | Likelihood -321.3926696777344 | KL Coef 0.9867219690399229\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 450 | Loss 227.18284606933594 | Likelihood -229.01602172851562 | KL Coef 0.9879915868082944\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 460 | Loss 200.6350860595703 | Likelihood -204.7289276123047 | KL Coef 0.9891398063601221\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 470 | Loss 182.41448974609375 | Likelihood -186.96951293945312 | KL Coef 0.9901782355409698\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 480 | Loss 193.407958984375 | Likelihood -208.0583038330078 | KL Coef 0.9911173722782946\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 490 | Loss 162.0176239013672 | Likelihood -167.3922119140625 | KL Coef 0.9919667107095133\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 500 | Loss 177.85520935058594 | Likelihood -185.06825256347656 | KL Coef 0.9927348371623237\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 510 | Loss 151.72157287597656 | Likelihood -156.14630126953125 | KL Coef 0.9934295169575854\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 520 | Loss 140.63656616210938 | Likelihood -143.12010192871094 | KL Coef 0.9940577729122909\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 530 | Loss 136.60171508789062 | Likelihood -138.36465454101562 | KL Coef 0.9946259563362442\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 540 | Loss 132.205078125 | Likelihood -133.1760711669922 | KL Coef 0.9951398112401846\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 550 | Loss 129.86585998535156 | Likelihood -130.3859405517578 | KL Coef 0.9956045324044637\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 560 | Loss 130.29525756835938 | Likelihood -131.35545349121094 | KL Coef 0.9960248178953148\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 570 | Loss 127.11035919189453 | Likelihood -127.75286102294922 | KL Coef 0.996404916559627\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 580 | Loss 125.7874984741211 | Likelihood -125.90291595458984 | KL Coef 0.9967486709783656\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 590 | Loss 123.61184692382812 | Likelihood -124.08000946044922 | KL Coef 0.997059556312878\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 600 | Loss 149.05506896972656 | Likelihood -153.31455993652344 | KL Coef 0.997340715436794\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 610 | Loss 124.12499237060547 | Likelihood -124.8366470336914 | KL Coef 0.997594990708689\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 620 | Loss 122.15506744384766 | Likelihood -122.0066909790039 | KL Coef 0.9978249527067087\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 630 | Loss 119.85746002197266 | Likelihood -120.17449951171875 | KL Coef 0.9980329262156509\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 640 | Loss 119.28223419189453 | Likelihood -119.30902099609375 | KL Coef 0.9982210137292149\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 650 | Loss 118.08660125732422 | Likelihood -117.90614318847656 | KL Coef 0.9983911167050152\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 660 | Loss 143.55406188964844 | Likelihood -147.43955993652344 | KL Coef 0.9985449547872347\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 670 | Loss 118.08889770507812 | Likelihood -118.7651596069336 | KL Coef 0.9986840831912477\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 680 | Loss 116.39049530029297 | Likelihood -117.07136535644531 | KL Coef 0.9988099084259616\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 690 | Loss 113.94464874267578 | Likelihood -114.43780517578125 | KL Coef 0.9989237025128207\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 700 | Loss 112.54154968261719 | Likelihood -113.68456268310547 | KL Coef 0.999026615845218\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 710 | Loss 111.46907043457031 | Likelihood -111.8253402709961 | KL Coef 0.9991196888183176\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 720 | Loss 110.31731414794922 | Likelihood -110.388916015625 | KL Coef 0.9992038623468565\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 730 | Loss 138.72695922851562 | Likelihood -140.14736938476562 | KL Coef 0.9992799873772575\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 740 | Loss 121.21542358398438 | Likelihood -120.27819061279297 | KL Coef 0.9993488334902116\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 750 | Loss 114.91764831542969 | Likelihood -114.1845932006836 | KL Coef 0.9994110966807014\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 760 | Loss 125.64954376220703 | Likelihood -134.65721130371094 | KL Coef 0.9994674063941131\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 770 | Loss 127.89332580566406 | Likelihood -133.32090759277344 | KL Coef 0.9995183318895716\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 780 | Loss 112.14657592773438 | Likelihood -112.2166748046875 | KL Coef 0.9995643879948252\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 790 | Loss 107.45929718017578 | Likelihood -109.88240814208984 | KL Coef 0.9996060403108612\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 800 | Loss 105.72601318359375 | Likelihood -107.0626220703125 | KL Coef 0.9996437099188669\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 810 | Loss 104.14693450927734 | Likelihood -104.7420883178711 | KL Coef 0.9996777776371197\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 820 | Loss 103.6618881225586 | Likelihood -104.10969543457031 | KL Coef 0.9997085878708442\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 830 | Loss 101.19972229003906 | Likelihood -101.45710754394531 | KL Coef 0.9997364520939512\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 840 | Loss 100.2546157836914 | Likelihood -100.70647430419922 | KL Coef 0.9997616519978635\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 850 | Loss 96.65178680419922 | Likelihood -96.92546844482422 | KL Coef 0.9997844423392536\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 860 | Loss 93.41980743408203 | Likelihood -93.7983169555664 | KL Coef 0.9998050535154901\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 870 | Loss 92.36575317382812 | Likelihood -93.12944793701172 | KL Coef 0.9998236938938232\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 880 | Loss 88.9665298461914 | Likelihood -90.39913177490234 | KL Coef 0.9998405519178591\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 890 | Loss 85.44792938232422 | Likelihood -85.82453918457031 | KL Coef 0.9998557980126173\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 900 | Loss 84.6084213256836 | Likelihood -85.31514739990234 | KL Coef 0.9998695863074304\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 910 | Loss 84.25326538085938 | Likelihood -85.7547378540039 | KL Coef 0.9998820561941043\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 920 | Loss 84.57292938232422 | Likelihood -84.83819580078125 | KL Coef 0.9998933337360897\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 930 | Loss 78.72087097167969 | Likelihood -78.79217529296875 | KL Coef 0.9999035329429113\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 940 | Loss 83.05723571777344 | Likelihood -83.8000717163086 | KL Coef 0.9999127569227402\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 950 | Loss 85.06168365478516 | Likelihood -85.7742691040039 | KL Coef 0.9999210989247576\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 960 | Loss 82.82308959960938 | Likelihood -82.873291015625 | KL Coef 0.9999286432818518\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 970 | Loss 96.3530044555664 | Likelihood -100.33917236328125 | KL Coef 0.9999354662631753\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 980 | Loss 81.57101440429688 | Likelihood -82.8602294921875 | KL Coef 0.9999416368451824\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 990 | Loss 77.8392105102539 | Likelihood -78.27254486083984 | KL Coef 0.9999472174089421\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1000 | Loss 75.0434799194336 | Likelihood -75.15254974365234 | KL Coef 0.9999522643707747\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1010 | Loss 74.85801696777344 | Likelihood -74.7017593383789 | KL Coef 0.9999568287525893\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1020 | Loss 72.45097351074219 | Likelihood -72.47785949707031 | KL Coef 0.999960956697686\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1030 | Loss 70.9055404663086 | Likelihood -71.20587921142578 | KL Coef 0.9999646899372381\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1040 | Loss 102.96393585205078 | Likelihood -102.95317840576172 | KL Coef 0.9999680662121707\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1050 | Loss 76.38431549072266 | Likelihood -76.21910858154297 | KL Coef 0.9999711196547001\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1060 | Loss 73.301513671875 | Likelihood -73.4366683959961 | KL Coef 0.9999738811333907\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1070 | Loss 85.22321319580078 | Likelihood -91.17980194091797 | KL Coef 0.9999763785652189\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1080 | Loss 80.49334716796875 | Likelihood -81.56978607177734 | KL Coef 0.999978637197798\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1090 | Loss 74.37738800048828 | Likelihood -76.46717834472656 | KL Coef 0.9999806798646166\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1100 | Loss 69.9637451171875 | Likelihood -69.83192443847656 | KL Coef 0.9999825272158726\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Epoch 1110 | Loss 67.316650390625 | Likelihood -67.5260009765625 | KL Coef 0.9999841979272346\n",
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter scale (Tensor of shape (1, 64, 20)) of distribution Normal(loc: torch.Size([1, 64, 20]), scale: torch.Size([1, 64, 20])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([[[0.1346, 0.0862, 0.0352,  ..., 0.2907, 0.5341, 0.0526],\n         [0.2618, 0.0149, 0.2634,  ..., 0.4181, 0.9398, 0.0369],\n         [0.0555, 0.0463, 0.0560,  ..., 0.5011, 0.7055, 0.0220],\n         ...,\n         [0.2551, 0.0506, 0.5747,  ..., 0.1992, 0.8548, 0.0110],\n         [0.1518, 0.0059, 0.1649,  ..., 0.4848, 0.7787, 0.0366],\n         [0.0592, 0.0796, 0.2870,  ..., 0.5021, 0.6303, 0.0081]]],\n       device='cuda:0', grad_fn=<AbsBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m kl_coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m itr \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_batches \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.99\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (itr \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_batches \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     37\u001b[0m batch_dict \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_next_batch(data_obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataloader\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 38\u001b[0m train_res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_all_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_traj_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkl_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m train_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/media/usr/SSD/intern/taejun/latent_ode/lib/base_models.py:264\u001b[0m, in \u001b[0;36mVAE_Baseline.compute_all_losses\u001b[0;34m(self, batch_dict, n_traj_samples, kl_coef)\u001b[0m\n\u001b[1;32m    262\u001b[0m fp_mu, fp_std, fp_enc \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_point\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    263\u001b[0m fp_std \u001b[38;5;241m=\u001b[39m fp_std\u001b[38;5;241m.\u001b[39mabs()\n\u001b[0;32m--> 264\u001b[0m fp_distr \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp_mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39msum(fp_std \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.\u001b[39m)\n\u001b[1;32m    268\u001b[0m kldiv_z0 \u001b[38;5;241m=\u001b[39m kl_divergence(fp_distr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz0_prior)\n",
      "File \u001b[0;32m~/anaconda3/envs/taejun/lib/python3.8/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/taejun/lib/python3.8/site-packages/torch/distributions/distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter scale (Tensor of shape (1, 64, 20)) of distribution Normal(loc: torch.Size([1, 64, 20]), scale: torch.Size([1, 64, 20])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([[[0.1346, 0.0862, 0.0352,  ..., 0.2907, 0.5341, 0.0526],\n         [0.2618, 0.0149, 0.2634,  ..., 0.4181, 0.9398, 0.0369],\n         [0.0555, 0.0463, 0.0560,  ..., 0.5011, 0.7055, 0.0220],\n         ...,\n         [0.2551, 0.0506, 0.5747,  ..., 0.1992, 0.8548, 0.0110],\n         [0.1518, 0.0059, 0.1649,  ..., 0.4848, 0.7787, 0.0366],\n         [0.0592, 0.0796, 0.2870,  ..., 0.5021, 0.6303, 0.0081]]],\n       device='cuda:0', grad_fn=<AbsBackward0>)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "experimentID = args.load if args.load is not None else int(SystemRandom().random() * 100000)\n",
    "ckpt_path = os.path.join(args.save, f\"experiment_{experimentID}.ckpt\")\n",
    "\n",
    "# 데이터 로딩 및 처리\n",
    "data_obj = data_objects # 'data_objects'는 데이터 로딩 및 전처리를 담당하는 코드 부분에서 정의됨\n",
    "input_dim = data_obj[\"input_dim\"]\n",
    "\n",
    "# 모델 생성\n",
    "obsrv_std = torch.Tensor([0.01]).to(device)\n",
    "z0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))\n",
    "\n",
    "if args.latent_ode:\n",
    "    model = create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device,\n",
    "                                   classif_per_tp=False, # 이 예제에서는 간소화를 위해 False로 설정\n",
    "                                   n_labels=1) # 이 예제에서는 단순화를 위해 1로 설정\n",
    "else:\n",
    "    raise Exception(\"Model not specified\")\n",
    "\n",
    "# # 로그 설정\n",
    "# log_path = f\"logs/{file_name}_{experimentID}.log\"\n",
    "# if not os.path.exists(\"logs/\"):\n",
    "#     os.makedirs(\"logs/\")\n",
    "# logger = utils.get_logger(logpath=log_path, filepath=os.path.abspath(__file__))\n",
    "# logger.info(\" \".join(sys.argv))\n",
    "\n",
    "# 최적화기 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# 훈련 과정\n",
    "num_batches = data_obj[\"n_train_batches\"]\n",
    "for itr in range(1, num_batches * (args.niters + 1)):\n",
    "    optimizer.zero_grad()\n",
    "    utils.update_learning_rate(optimizer, decay_rate=0.999, lowest=args.lr / 10)\n",
    "\n",
    "    kl_coef = 0. if itr // num_batches < 10 else (1-0.99 ** (itr // num_batches - 10))\n",
    "\n",
    "    batch_dict = utils.get_next_batch(data_obj[\"train_dataloader\"])\n",
    "    train_res = model.compute_all_losses(batch_dict, n_traj_samples=10, kl_coef=kl_coef)\n",
    "    train_res[\"loss\"].backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 주기적 로깅\n",
    "    if itr % (10 * num_batches) == 0:\n",
    "        print(f\"Epoch {itr // num_batches} | Loss {train_res['loss'].item()} | Likelihood {train_res['likelihood'].item()} | KL Coef {kl_coef}\")\n",
    "\n",
    "        # 훈련 데이터 시각화 (선택적)\n",
    "        if args.viz:  # 시각화 옵션이 활성화된 경우\n",
    "            plot_name_train = f\"final_real_lab_data/train_finalresult_20_{itr // num_batches:04d}\"\n",
    "            with torch.no_grad():\n",
    "                Visualizations(device).draw_all_plots_one_dim(batch_dict, model.to(device), plot_name=plot_name_train, save=True)\n",
    "            with torch.no_grad():\n",
    "                test_res = compute_loss_all_batches(model, \n",
    "\t\t\t\t\tdata_obj[\"test_dataloader\"], args,\n",
    "\t\t\t\t\tn_batches = data_obj[\"n_test_batches\"],\n",
    "\t\t\t\t\texperimentID = experimentID,\n",
    "\t\t\t\t\tdevice = device,\n",
    "\t\t\t\t\tn_traj_samples = 10, kl_coef = kl_coef)\n",
    "                sample_test = utils.get_next_batch(data_obj[\"test_dataloader\"])\n",
    "                plot_name_test = f\"final_real_lab_data/test_finalresult_20_{itr // num_batches:04d}\"\n",
    "                Visualizations(device).draw_all_plots_one_dim(sample_test ,model.to(device), plot_name=plot_name_test, save = True)\n",
    "\t\t\t\t\n",
    "\n",
    "# 체크포인트 저장 (선택적)\n",
    "# torch.save(model.state_dict(), ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n",
      "Computing loss... 1\n",
      "Computing loss... 2\n",
      "Computing loss... 3\n",
      "Final Test Loss Summary:\n",
      "loss: 81.40855407714844\n",
      "likelihood: -81.25747680664062\n",
      "mse: 0.016988741233944893\n",
      "kl_first_p: 2.939375400543213\n",
      "std_first_p: 0.22200098633766174\n",
      "pois_likelihood: 0.0\n",
      "ce_loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    final_test_res = compute_loss_all_batches(model, \n",
    "                    data_obj[\"test_dataloader\"], args,\n",
    "                    n_batches=data_obj[\"n_test_batches\"],\n",
    "                    experimentID=experimentID,\n",
    "                    device=device,\n",
    "                    n_traj_samples=10, kl_coef=kl_coef)\n",
    "\n",
    "    print(\"Final Test Loss Summary:\")\n",
    "    for key, value in final_test_res.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "# \ttorch.manual_seed(args.random_seed)\n",
    "# \tnp.random.seed(args.random_seed)\n",
    "\n",
    "# \texperimentID = args.load\n",
    "# \tif experimentID is None:\n",
    "# \t\t# Make a new experiment ID\n",
    "# \t\texperimentID = int(SystemRandom().random()*100000)\n",
    "# \tckpt_path = os.path.join(args.save, \"experiment_\" + str(experimentID) + '.ckpt')\n",
    "\n",
    "# \tstart = time.time()\n",
    "# \tprint(\"Sampling dataset of {} training examples\".format(args.n))\n",
    "\t\n",
    "# \tinput_command = sys.argv\n",
    "# \tind = [i for i in range(len(input_command)) if input_command[i] == \"--load\"]\n",
    "# \tif len(ind) == 1:\n",
    "# \t\tind = ind[0]\n",
    "# \t\tinput_command = input_command[:ind] + input_command[(ind+2):]\n",
    "# \tinput_command = \" \".join(input_command)\n",
    "\n",
    "# \tutils.makedirs(\"results/\")\n",
    "\n",
    "# \t##################################################################\n",
    "# \tdata_obj = data_objects\n",
    "# \tinput_dim = data_obj[\"input_dim\"]\n",
    "\n",
    "# \tclassif_per_tp = False\n",
    "# \tif (\"classif_per_tp\" in data_obj):\n",
    "# \t\t# do classification per time point rather than on a time series as a whole\n",
    "# \t\tclassif_per_tp = data_obj[\"classif_per_tp\"]\n",
    "\n",
    "# \tif args.classif and (args.dataset == \"hopper\" or args.dataset == \"periodic\"):\n",
    "# \t\traise Exception(\"Classification task is not available for MuJoCo and 1d datasets\")\n",
    "\n",
    "# \tn_labels = 1\n",
    "# \tif args.classif:\n",
    "# \t\tif (\"n_labels\" in data_obj):\n",
    "# \t\t\tn_labels = data_obj[\"n_labels\"]\n",
    "# \t\telse:\n",
    "# \t\t\traise Exception(\"Please provide number of labels for classification task\")\n",
    "\n",
    "# \t##################################################################\n",
    "# \t# Create the model\n",
    "# \tobsrv_std = 0.01\n",
    "# \tobsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
    "# \tz0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))\n",
    "\n",
    "# \tif args.rnn_vae:\n",
    "# \t\tif args.poisson:\n",
    "# \t\t\tprint(\"Poisson process likelihood not implemented for RNN-VAE: ignoring --poisson\")\n",
    "\n",
    "# \t\t# Create RNN-VAE model\n",
    "# \t\tmodel = RNN_VAE(input_dim, args.latents, \n",
    "# \t\t\tdevice = device, \n",
    "# \t\t\trec_dims = args.rec_dims, \n",
    "# \t\t\tconcat_mask = True, \n",
    "# \t\t\tobsrv_std = obsrv_std,\n",
    "# \t\t\tz0_prior = z0_prior,\n",
    "# \t\t\tuse_binary_classif = args.classif,\n",
    "# \t\t\tclassif_per_tp = classif_per_tp,\n",
    "# \t\t\tlinear_classifier = args.linear_classif,\n",
    "# \t\t\tn_units = args.units,\n",
    "# \t\t\tinput_space_decay = args.input_decay,\n",
    "# \t\t\tcell = args.rnn_cell,\n",
    "# \t\t\tn_labels = n_labels,\n",
    "# \t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "# \t\t\t).to(device)\n",
    "\t\t\n",
    "# \telif args.classic_rnn:\n",
    "# \t\tif args.poisson:\n",
    "# \t\t\tprint(\"Poisson process likelihood not implemented for RNN: ignoring --poisson\")\n",
    "\n",
    "# \t\tif args.extrap:\n",
    "# \t\t\traise Exception(\"Extrapolation for standard RNN not implemented\")\n",
    "# \t\t# Create RNN model\n",
    "# \t\tmodel = Classic_RNN(input_dim, args.latents, device, \n",
    "# \t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
    "# \t\t\tn_units = args.units,\n",
    "# \t\t\tuse_binary_classif = args.classif,\n",
    "# \t\t\tclassif_per_tp = classif_per_tp,\n",
    "# \t\t\tlinear_classifier = args.linear_classif,\n",
    "# \t\t\tinput_space_decay = args.input_decay,\n",
    "# \t\t\tcell = args.rnn_cell,\n",
    "# \t\t\tn_labels = n_labels,\n",
    "# \t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "# \t\t\t).to(device)\n",
    "\t\t\n",
    "# \telif args.ode_rnn:\n",
    "# \t\t# Create ODE-GRU model\n",
    "# \t\tn_ode_gru_dims = args.latents\n",
    "\t\t\t\t\n",
    "# \t\tif args.poisson:\n",
    "# \t\t\tprint(\"Poisson process likelihood not implemented for ODE-RNN: ignoring --poisson\")\n",
    "\n",
    "# \t\tif args.extrap:\n",
    "# \t\t\traise Exception(\"Extrapolation for ODE-RNN not implemented\")\n",
    "\n",
    "# \t\tode_func_net = utils.create_net(n_ode_gru_dims, n_ode_gru_dims, \n",
    "# \t\t\tn_layers = args.rec_layers, n_units = args.units, nonlinear = nn.Tanh)\n",
    "\n",
    "# \t\trec_ode_func = ODEFunc(\n",
    "# \t\t\tinput_dim = input_dim, \n",
    "# \t\t\tlatent_dim = n_ode_gru_dims,\n",
    "# \t\t\tode_func_net = ode_func_net,\n",
    "# \t\t\tdevice = device).to(device)\n",
    "\n",
    "# \t\tz0_diffeq_solver = DiffeqSolver(input_dim, rec_ode_func, \"euler\", args.latents, \n",
    "# \t\t\todeint_rtol = 1e-3, odeint_atol = 1e-4, device = device)\n",
    "\t\n",
    "# \t\tmodel = ODE_RNN(input_dim, n_ode_gru_dims, device = device, \n",
    "# \t\t\tz0_diffeq_solver = z0_diffeq_solver, n_gru_units = args.gru_units,\n",
    "# \t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
    "# \t\t\tuse_binary_classif = args.classif,\n",
    "# \t\t\tclassif_per_tp = classif_per_tp,\n",
    "# \t\t\tn_labels = n_labels,\n",
    "# \t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "# \t\t\t).to(device)\n",
    "# \telif args.latent_ode:\n",
    "# \t\tmodel = create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device, \n",
    "# \t\t\tclassif_per_tp = classif_per_tp,\n",
    "# \t\t\tn_labels = n_labels)\n",
    "# \telse:\n",
    "# \t\traise Exception(\"Model not specified\")\n",
    "\n",
    "# \t##################################################################\n",
    "\n",
    "# \tif args.viz:\n",
    "# \t\tviz = Visualizations(device)\n",
    "\n",
    "# \t##################################################################\n",
    "\t\n",
    "# \t#Load checkpoint and evaluate the model\n",
    "# \tif args.load is not None:\n",
    "# \t\tutils.get_ckpt_model(ckpt_path, model, device)\n",
    "# \t\texit()\n",
    "\n",
    "# \t##################################################################\n",
    "# \t# Training\n",
    "\n",
    "# \tlog_path = \"logs/\" + file_name + \"_\" + str(experimentID) + \".log\"\n",
    "# \tif not os.path.exists(\"logs/\"):\n",
    "# \t\tutils.makedirs(\"logs/\")\n",
    "# \tlogger = utils.get_logger(logpath=log_path, filepath=os.path.abspath(file_name))\n",
    "# \tlogger.info(input_command)\n",
    "\n",
    "# \toptimizer = optim.Adamax(model.parameters(), lr=args.lr)\n",
    "\n",
    "# \tnum_batches = data_obj[\"n_train_batches\"]\n",
    "\n",
    "# \tfor itr in range(1, num_batches * (args.niters + 1)):\n",
    "# \t\toptimizer.zero_grad()\n",
    "# \t\tutils.update_learning_rate(optimizer, decay_rate=0.999, lowest=args.lr / 10)\n",
    "\n",
    "# \t\twait_until_kl_inc = 10\n",
    "# \t\tif itr // num_batches < wait_until_kl_inc:\n",
    "# \t\t\tkl_coef = 0.\n",
    "# \t\telse:\n",
    "# \t\t\tkl_coef = (1-0.99**(itr // num_batches - wait_until_kl_inc))\n",
    "\n",
    "# \t\tbatch_dict = utils.get_next_batch(data_obj[\"train_dataloader\"])\n",
    "# \t\tprint(batch_dict['observed_data'])\n",
    "# \t\ttrain_res = model.compute_all_losses(batch_dict, n_traj_samples=10, kl_coef=kl_coef)\n",
    "# \t\ttrain_res[\"loss\"].backward()\n",
    "# \t\toptimizer.step()\n",
    "\n",
    "# \t\t# 주기적으로 로깅 및 시각화를 위한 조건문\n",
    "# \t\tif itr % (10 * num_batches) == 0:  # 예를 들어, 10 epoch 마다 로그와 시각화 수행\n",
    "# \t\t\tlogger.info(f\"Epoch {itr // num_batches} | Loss {train_res['loss'].item()} | Likelihood {train_res['likelihood'].item()} | KL Coef {kl_coef}\")\n",
    "\n",
    "# \t\t\t# 훈련 데이터 시각화\n",
    "# \t\t\tplot_name_train = f\"train_result_{itr // num_batches:04d}\"\n",
    "# \t\t\twith torch.no_grad():  # 시각화 동안에는 기울기 계산을 하지 않음\n",
    "# \t\t\t\tVisualizations(device).draw_all_plots_one_dim(batch_dict, model.to(device), plot_name=plot_name_train, save=True)\n",
    "\n",
    "# \t\t\t# 추가 정보 로깅\n",
    "# \t\t\tlogger.info(f\"Train loss (one batch): {train_res['loss'].item()}\")\n",
    "# \t\t\tif 'ce_loss' in train_res:\n",
    "# \t\t\t\tlogger.info(f\"Train CE loss (one batch): {train_res['ce_loss'].item()}\")\n",
    "\n",
    "\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\t\t\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 523, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dict['observed_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object inf_generator at 0x7fcfd36ec190>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_obj[\"test_dataloader\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taejun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
