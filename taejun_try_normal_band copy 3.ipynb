{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import SystemRandom\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "import torch.optim as optim\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.plotting import *\n",
    "\n",
    "from lib.rnn_baselines import *\n",
    "from lib.ode_rnn import *\n",
    "from lib.create_latent_ode_model import create_LatentODE_model\n",
    "from lib.parse_datasets import parse_datasets\n",
    "from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
    "from lib.diffeq_solver import DiffeqSolver\n",
    "from mujoco_physics import HopperPhysics\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.diffeq_solver import DiffeqSolver\n",
    "from generate_timeseries import Periodic_1d\n",
    "from torch.distributions import uniform\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from mujoco_physics import HopperPhysics\n",
    "from physionet import variable_time_collate_fn, get_data_min_max\n",
    "from person_activity import PersonActivity, variable_time_collate_fn_activity\n",
    "\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from lib.utils import compute_loss_all_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative model for noisy data based on ODE\n",
    "parser = argparse.ArgumentParser('Latent ODE')\n",
    "parser.add_argument('-n',  type=int, default=10000, help=\"Size of the dataset\")\n",
    "parser.add_argument('--niters', type=int, default=20)\n",
    "parser.add_argument('--lr',  type=float, default=1e-2, help=\"Starting learning rate.\")\n",
    "parser.add_argument('-b', '--batch-size', type=int, default=256)\n",
    "parser.add_argument('--viz', action='store_true', default=False, help=\"Show plots while training\")\n",
    "\n",
    "parser.add_argument('--save', type=str, default='experiments/', help=\"Path for save checkpoints\")\n",
    "parser.add_argument('--load', type=str, default=None, help=\"ID of the experiment to load for evaluation. If None, run a new experiment.\")\n",
    "parser.add_argument('-r', '--random-seed', type=int, default=1991, help=\"Random_seed\")\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default='physionet', help=\"Dataset to load. Available: physionet, activity, hopper, periodic\")\n",
    "parser.add_argument('-s', '--sample-tp', type=float, default=None, help=\"Number of time points to sub-sample.\"\n",
    "\t\"If > 1, subsample exact number of points. If the number is in [0,1], take a percentage of available points per time series. If None, do not subsample\")\n",
    "\n",
    "parser.add_argument('-c', '--cut-tp', type=int, default=None, help=\"Cut out the section of the timeline of the specified length (in number of points).\"\n",
    "\t\"Used for periodic function demo.\")\n",
    "\n",
    "parser.add_argument('--quantization', type=float, default=5, help=\"Quantization on the physionet dataset.\"\n",
    "\t\"Value 1 means quantization by 1 hour, value 0.1 means quantization by 0.1 hour = 6 min\")\n",
    "\n",
    "parser.add_argument('--latent-ode', action='store_true', default=True,  help=\"Run Latent ODE seq2seq model\")\n",
    "parser.add_argument('--z0-encoder', type=str, default='odernn', help=\"Type of encoder for Latent ODE model: odernn or rnn\")\n",
    "\n",
    "parser.add_argument('--classic-rnn', action='store_true', help=\"Run RNN baseline: classic RNN that sees true points at every point. Used for interpolation only.\")\n",
    "parser.add_argument('--rnn-cell', default=\"gru\", help=\"RNN Cell type. Available: gru (default), expdecay\")\n",
    "parser.add_argument('--input-decay', action='store_true', help=\"For RNN: use the input that is the weighted average of impirical mean and previous value (like in GRU-D)\")\n",
    "\n",
    "parser.add_argument('--ode-rnn', action='store_true', help=\"Run ODE-RNN baseline: RNN-style that sees true points at every point. Used for interpolation only.\")\n",
    "\n",
    "parser.add_argument('--rnn-vae', action='store_true', help=\"Run RNN baseline: seq2seq model with sampling of the h0 and ELBO loss.\")\n",
    "\n",
    "parser.add_argument('-l', '--latents', type=int, default=6, help=\"Size of the latent state\")\n",
    "parser.add_argument('--rec-dims', type=int, default=40, help=\"Dimensionality of the recognition model (ODE or RNN).\")\n",
    "\n",
    "parser.add_argument('--rec-layers', type=int, default=3, help=\"Number of layers in ODE func in recognition ODE\")\n",
    "parser.add_argument('--gen-layers', type=int, default=3, help=\"Number of layers in ODE func in generative ODE\")\n",
    "\n",
    "parser.add_argument('-u', '--units', type=int, default=50, help=\"Number of units per layer in ODE func\")\n",
    "parser.add_argument('-g', '--gru-units', type=int, default=100, help=\"Number of units per layer in each of GRU update networks\")\n",
    "\n",
    "parser.add_argument('--poisson', action='store_true', help=\"Model poisson-process likelihood for the density of events in addition to reconstruction.\")\n",
    "parser.add_argument('--classif', action='store_true', help=\"Include binary classification loss -- used for Physionet dataset for hospiral mortality\")\n",
    "\n",
    "parser.add_argument('--linear-classif', action='store_true', help=\"If using a classifier, use a linear classifier instead of 1-layer NN\")\n",
    "parser.add_argument('--extrap', action='store_true', help=\"Set extrapolation mode. If this flag is not set, run interpolation mode.\")\n",
    "\n",
    "parser.add_argument('-t', '--timepoints', type=int, default=100, help=\"Total number of time-points\")\n",
    "parser.add_argument('--max-t',  type=float, default=5., help=\"We subsample points in the interval [0, args.max_tp]\")\n",
    "parser.add_argument('--noise-weight', type=float, default=0.01, help=\"Noise amplitude for generated traejctories\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.n = 10000\n",
    "        self.niters = 1000\n",
    "        self.lr = 1e-2\n",
    "        self.batch_size = 128\n",
    "        self.viz = True\n",
    "        self.save = 'experiments/'\n",
    "        self.load = None\n",
    "        self.random_seed = 1991\n",
    "        self.dataset = 'physionet'\n",
    "        self.sample_tp = 0.6\n",
    "        self.cut_tp = None\n",
    "        self.quantization = 5\n",
    "        self.latent_ode = True\n",
    "        self.z0_encoder = 'odernn'\n",
    "        self.classic_rnn = False\n",
    "        self.rnn_cell = \"gru\"\n",
    "        self.input_decay = False\n",
    "        self.ode_rnn = False\n",
    "        self.rnn_vae = False\n",
    "        self.latents = 20\n",
    "        self.rec_dims = 10\n",
    "        self.rec_layers = 5\n",
    "        self.gen_layers = 5\n",
    "        self.units = 50\n",
    "        self.gru_units = 100\n",
    "        self.poisson = False\n",
    "        self.classif = False\n",
    "        self.linear_classif = False\n",
    "        self.extrap = False\n",
    "        self.timepoints = 100\n",
    "        self.max_t = 5.\n",
    "        self.noise_weight = 0.01\n",
    "\n",
    "# args 객체를 생성하고 필요한 설정을 할당합니다\n",
    "args = Args()\n",
    "\n",
    "# 몇 가지 설정을 수정합니다\n",
    "args.batch_size = 64\n",
    "args.classif = False\n",
    "args.quantization = 5\n",
    "args.niters = 1000\n",
    "args.n = 100\n",
    "args.sample_tp = 0.6\n",
    "args.latents = 20\n",
    "args.rec_dims = 10\n",
    "args.rec_layers = 5\n",
    "args.gen_layers = 5\n",
    "args.latent_ode = True\n",
    "args.viz = True\n",
    "args.max_t = 200\n",
    "\n",
    "# 사용할 디바이스를 설정합니다\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 파일 이름을 설정합니다\n",
    "file_name = 'taejun_sim_'\n",
    "\n",
    "# 설정을 확인합니다\n",
    "print(\"Batch size:\", args.batch_size)\n",
    "print(\"Number of iterations:\", args.niters)\n",
    "print(\"Learning rate:\", args.lr)\n",
    "print(\"Device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, classic_rnn=False, classif=False, cut_tp=None, dataset='physionet', extrap=False, gen_layers=5, gru_units=100, input_decay=False, l=20, latent_ode=True, latents=6, linear_classif=False, load=None, lr=0.01, max_t=5.0, n=2, niters=1000, noise_weight=0.01, ode_rnn=False, poisson=False, quantization=5, random_seed=1991, rec_dims=10, rec_layers=5, rnn_cell='gru', rnn_vae=False, s=30, sample_tp=None, save='experiments/', timepoints=100, units=50, viz=True, z0_encoder='odernn')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from jdcal import jd2gcal\n",
    "from datetime import datetime\n",
    "\n",
    "class CustomClass(object):\n",
    "    params = ['Magnitude']  # Uncertainty_of_Magnitude 제외\n",
    "\n",
    "    params_dict = {k: i for i, k in enumerate(params)}\n",
    "\n",
    "    def __init__(self, root, train=True, preprocess=False,\n",
    "                 quantization=1, n_samples=None, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.quantization = quantization\n",
    "        self.device = device\n",
    "        self.reduce = \"average\"\n",
    "        if preprocess:\n",
    "            self.preprocess()\n",
    "\n",
    "        self.data = torch.load(os.path.join(self.root, 'lc_' + str(self.quantization) + '.pt'))\n",
    "        self.labels = torch.zeros(len(self.data))\n",
    "\n",
    "        if n_samples is not None:\n",
    "            self.data = self.data[:n_samples]\n",
    "            self.labels = self.labels[:n_samples]\n",
    "\n",
    "    def jd_to_total_hours(self,jd):\n",
    "        jd_int = int(jd)\n",
    "        jd_frac = jd - jd_int\n",
    "        # jd2gcal 함수를 사용하여 율리우스 일을 그레고리언 날짜로 변환\n",
    "        # 이 예에서는 jd2gcal 함수의 정의나 import 방법이 제공되지 않았으므로, 해당 함수의 정확한 동작을 가정합니다.\n",
    "        year, month, day, fraction = jd2gcal(jd_int, jd_frac)\n",
    "        # 연도 계산이 특정 요구 사항에 맞게 조정되어 있습니다.\n",
    "        year = (year + 4553) + 2000\n",
    "        hours = int(fraction * 24)\n",
    "        converted_datetime = datetime(year, month, day, hours)\n",
    "        unix_start = datetime(1970, 1, 1)\n",
    "        total_hours = int((converted_datetime - unix_start).total_seconds() / 86400)\n",
    "        return total_hours\n",
    "\n",
    "    def preprocess(self):\n",
    "        simulation_data_root = self.root \n",
    "        data_list = [f for f in os.listdir(simulation_data_root) if f.endswith('.lc')]\n",
    "        data_list.sort()\n",
    "        light_curves = []\n",
    "\n",
    "        for name in tqdm(data_list):\n",
    "            lc_name = name.partition('.')[0]\n",
    "            with open(os.path.join(simulation_data_root, name)) as f:\n",
    "                # next(f)  # 헤더 건너뛰기\n",
    "                magnitudes = [float(line.rstrip().split(' ')[1]) for line in f]\n",
    "                global_min = min(magnitudes)\n",
    "                global_max = max(magnitudes)\n",
    "\n",
    "            # 파일을 다시 열어서 데이터 처리\n",
    "            with open(os.path.join(simulation_data_root, name)) as f:\n",
    "                next(f)\n",
    "                lines = f.readlines()\n",
    "                prev_time = 0\n",
    "                tt = [0.]\n",
    "                vals = [torch.zeros(len(self.params), device=self.device)]\n",
    "                mask = [torch.zeros(len(self.params), device=self.device)]\n",
    "                for line in lines:\n",
    "                    time, magnitude = line.rstrip().split(' ')[:2]\n",
    "                    time = float(time)\n",
    "                    magnitude = float(magnitude)\n",
    "                    time = self.jd_to_total_hours(time)\n",
    "\n",
    "                    # 파일별 최소/최대값으로 스케일링\n",
    "                    scaled_magnitude = (magnitude - global_min) / (global_max - global_min) if global_max > global_min else 0.0\n",
    "\n",
    "                    if time != prev_time:\n",
    "                        tt.append(time)\n",
    "                        vals.append(torch.zeros(len(self.params), device=self.device))\n",
    "                        mask.append(torch.zeros(len(self.params), device=self.device))\n",
    "                        prev_time = time\n",
    "\n",
    "                    vals[-1][0] = scaled_magnitude  # 스케일된 값 사용\n",
    "                    mask[-1][0] = 1\n",
    "\n",
    "            tt = torch.tensor(tt, device=self.device)[1:]\n",
    "            vals = torch.stack(vals)[1:]\n",
    "            mask = torch.stack(mask)[1:]\n",
    "            labels = None\n",
    "            light_curves.append((lc_name, tt, vals, mask, labels))\n",
    "\n",
    "        torch.save(light_curves, os.path.join(self.root, 'lc_' + str(self.quantization) + '.pt'))\n",
    "        print('Done!')\n",
    "\n",
    "\n",
    "\n",
    "    # def preprocess(self):\n",
    "    #     simulation_data_root = self.root \n",
    "    #     data_list = [f for f in os.listdir(simulation_data_root) if f.endswith('.txt')]\n",
    "    #     data_list.sort()\n",
    "    #     light_curves = []\n",
    "\n",
    "    #     # 전체 데이터셋에 대한 최소값과 최대값을 초기화\n",
    "    #     global_min = float('inf')\n",
    "    #     global_max = float('-inf')\n",
    "\n",
    "    #     # 최소값과 최대값 찾기\n",
    "    #     for name in tqdm(data_list):\n",
    "    #         with open(os.path.join(simulation_data_root, name)) as f:\n",
    "    #             next(f)  # 헤더 건너뛰기\n",
    "    #             for line in f:\n",
    "    #                 _, magnitude = line.rstrip().split(' ')[:2]\n",
    "    #                 magnitude = float(magnitude)\n",
    "    #                 global_min = min(global_min, magnitude)\n",
    "    #                 global_max = max(global_max, magnitude)\n",
    "\n",
    "    #     # 데이터 스케일링 및 처리\n",
    "    #     for name in tqdm(data_list):\n",
    "    #         lc_name = name.partition('.')[0]\n",
    "    #         with open(os.path.join(simulation_data_root, name)) as f:\n",
    "    #             next(f)\n",
    "    #             lines = f.readlines()\n",
    "    #             prev_time = 0\n",
    "    #             tt = [0.]\n",
    "    #             vals = [torch.zeros(len(self.params), device=self.device)]\n",
    "    #             mask = [torch.zeros(len(self.params), device=self.device)]\n",
    "    #             for line in lines:\n",
    "    #                 time, magnitude = line.rstrip().split(' ')[:2]\n",
    "    #                 time = float(time)\n",
    "    #                 magnitude = float(magnitude)\n",
    "\n",
    "    #                 # 스케일링: (magnitude - global_min) / (global_max - global_min)\n",
    "    #                 scaled_magnitude = (magnitude - global_min) / (global_max - global_min)\n",
    "\n",
    "    #                 if time != prev_time:\n",
    "    #                     tt.append(time)\n",
    "    #                     vals.append(torch.zeros(len(self.params), device=self.device))\n",
    "    #                     mask.append(torch.zeros(len(self.params), device=self.device))\n",
    "    #                     prev_time = time\n",
    "\n",
    "    #                 vals[-1][0] = scaled_magnitude  # 스케일된 값 사용\n",
    "    #                 mask[-1][0] = 1\n",
    "\n",
    "    #         tt = torch.tensor(tt, device=self.device)[1:]\n",
    "    #         vals = torch.stack(vals)[1:]\n",
    "    #         mask = torch.stack(mask)[1:]\n",
    "    #         labels = None\n",
    "    #         light_curves.append((lc_name, tt, vals, mask, labels))\n",
    "\n",
    "    #     torch.save(light_curves, os.path.join(self.root, 'lc_' + str(self.quantization) + '.pt'))\n",
    "    #     print('Done!')\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        fmt_str += '    Split: {}\\n'.format('train' if self.train else 'test')\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        fmt_str += '    Quantization: {}\\n'.format(self.quantization)\n",
    "        fmt_str += '    Reduce: {}\\n'.format(self.reduce)\n",
    "        return fmt_str\n",
    "\n",
    "    def visualize(self, timesteps, data, mask, plot_name):\n",
    "        width = 15\n",
    "        height = 15\n",
    "\n",
    "        non_zero_attributes = (torch.sum(mask,0) > 2).numpy()\n",
    "        non_zero_idx = [i for i in range(len(non_zero_attributes)) if non_zero_attributes[i] == 1.]\n",
    "        n_non_zero = sum(non_zero_attributes)\n",
    "\n",
    "        mask = mask[:, non_zero_idx]\n",
    "        data = data[:, non_zero_idx]\n",
    "\n",
    "        params_non_zero = [self.params[i] for i in non_zero_idx]\n",
    "        params_dict = {k: i for i, k in enumerate(params_non_zero)}\n",
    "\n",
    "        n_col = 3\n",
    "        n_row = n_non_zero // n_col + (n_non_zero % n_col > 0)\n",
    "        fig, ax_list = plt.subplots(n_row, n_col, figsize=(width, height), facecolor='white')\n",
    "\n",
    "        #for i in range(len(self.params)):\n",
    "        for i in range(n_non_zero):\n",
    "            param = params_non_zero[i]\n",
    "            param_id = params_dict[param]\n",
    "\n",
    "            tp_mask = mask[:,param_id].long()\n",
    "\n",
    "            tp_cur_param = timesteps[tp_mask == 1.]\n",
    "            data_cur_param = data[tp_mask == 1., param_id]\n",
    "\n",
    "            ax_list[i // n_col, i % n_col].plot(tp_cur_param.numpy(), data_cur_param.numpy(),  marker='o') \n",
    "            ax_list[i // n_col, i % n_col].set_title(param)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(plot_name)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "            # 예제 사용\n",
    "            # device 설정: CUDA 사용 가능한 경우 CUDA 사용, 그렇지 않으면 CPU 사용\n",
    "            #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # CustomClass 객체 생성 예제\n",
    "            #simulation_sdss = CustomClass(root='/home/intern/SSD/intern/taejun/test', train=True, preprocess=True,quantization=1, device=device)\n",
    "\n",
    "            # 객체 생성 시, preprocess=True로 설정하여 데이터 전처리 및 저장이 이루어집니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드에 구현된거 하지만 잘 안된다 수정해서 해보자\n",
    "def variable_time_collate_fn(batch, args, device = device, data_type = \"train\", \n",
    "\tdata_min = None, data_max = None):\n",
    "\t\"\"\"\n",
    "\tExpects a batch of time series data in the form of (record_id, tt, vals, mask, labels) where\n",
    "\t\t- record_id is a patient id\n",
    "\t\t- tt is a 1-dimensional tensor containing T time values of observations.\n",
    "\t\t- vals is a (T, D) tensor containing observed values for D variables.\n",
    "\t\t- mask is a (T, D) tensor containing 1 where values were observed and 0 otherwise.\n",
    "\t\t- labels is a list of labels for the current patient, if labels are available. Otherwise None.\n",
    "\tReturns:\n",
    "\t\tcombined_tt: The union of all time observations.\n",
    "\t\tcombined_vals: (M, T, D) tensor containing the observed values.\n",
    "\t\tcombined_mask: (M, T, D) tensor containing 1 where values were observed and 0 otherwise.\n",
    "\t\"\"\"\n",
    "\tD = batch[0][2].shape[1]\n",
    "\tcombined_tt, inverse_indices = torch.unique(torch.cat([ex[1] for ex in batch]), sorted=True, return_inverse=True)\n",
    "\tcombined_tt = combined_tt.to(device)\n",
    "\n",
    "\toffset = 0\n",
    "\tcombined_vals = torch.zeros([len(batch), len(combined_tt), D]).to(device)\n",
    "\tcombined_mask = torch.zeros([len(batch), len(combined_tt), D]).to(device)\n",
    "\t\n",
    "\tcombined_labels = None\n",
    "\tN_labels = 1\n",
    "\n",
    "\tcombined_labels = torch.zeros(len(batch), N_labels) + torch.tensor(float('nan'))\n",
    "\tcombined_labels = combined_labels.to(device = device)\n",
    "\t\n",
    "\tfor b, (record_id, tt, vals, mask, labels) in enumerate(batch):\n",
    "\t\ttt = tt.to(device)\n",
    "\t\tvals = vals.to(device)\n",
    "\t\tmask = mask.to(device)\n",
    "\t\tif labels is not None:\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\tindices = inverse_indices[offset:offset + len(tt)]\n",
    "\t\toffset += len(tt)\n",
    "\n",
    "\t\tcombined_vals[b, indices] = vals\n",
    "\t\tcombined_mask[b, indices] = mask\n",
    "\n",
    "\t\tif labels is not None:\n",
    "\t\t\tcombined_labels[b] = labels\n",
    "\n",
    "\tcombined_vals, _, _ = utils.normalize_masked_data(combined_vals, combined_mask, \n",
    "\t  \tatt_min = data_min, att_max = data_max)\n",
    "\n",
    "\t# if torch.max(combined_tt) != 0.:\n",
    "\t# \tcombined_tt = combined_tt / torch.max(combined_tt)\n",
    "\t\n",
    "\tdata_dict = {\n",
    "\t\t\"data\": combined_vals, \n",
    "\t\t\"time_steps\": combined_tt,\n",
    "\t\t\"mask\": combined_mask,\n",
    "\t\t\"labels\": combined_labels}\n",
    "\t\n",
    "\tdata_dict = utils.split_and_subsample_batch(data_dict, args, data_type = data_type)\n",
    "\treturn data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_batch = next(iter(train_dataloader))\n",
    "\n",
    "# # 첫 번째 배치의 실제 내용을 출력하여 어떤 키가 있는지 확인\n",
    "# print(first_batch.keys())\n",
    "\n",
    "# # 첫 번째 배치의 구성요소들의 shape를 출력\n",
    "# print(\"observed_data:\", first_batch['observed_data'].shape)\n",
    "# print(\"observed_tp:\", first_batch['observed_tp'].shape)\n",
    "# print(\"data_to_predict:\", first_batch['data_to_predict'].shape)\n",
    "# print(\"tp_to_predict:\", first_batch['tp_to_predict'].shape)\n",
    "# print(\"observed_mask:\", first_batch['observed_mask'].shape)\n",
    "# print(\"mask_predicted_data\", first_batch['mask_predicted_data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_zeros = torch.sum(first_batch['observed_mask']== 0).item()  # 마스크에서 0의 개수\n",
    "# num_ones = torch.sum(first_batch['observed_mask']== 1).item()\n",
    "\n",
    "# print(num_zeros)\n",
    "# print(num_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DataLoader에서 첫 번째 배치 데이터를 가져옴\n",
    "# batch_data = next(iter(train_dataloader))\n",
    "# print(batch_data.keys())\n",
    "# # 마스킹된 데이터 확인\n",
    "# mask = batch_data[\"mask\"]  # \"mask\" 키를 사용하여 마스크 텐서를 가져옴\n",
    "\n",
    "# # 마스크 텐서에서 0과 1의 개수를 세어서 출력\n",
    "# num_zeros = torch.sum(mask == 0).item()  # 마스크에서 0의 개수\n",
    "# num_ones = torch.sum(mask == 1).item()  # 마스크에서 1의 개수\n",
    "\n",
    "# print(f\"마스크에서 0의 개수: {num_zeros}\")\n",
    "# print(f\"마스크에서 1의 개수: {num_ones}\")\n",
    "\n",
    "# # 옵셔널: 마스킹된 데이터 시각화\n",
    "# # 데이터와 마스크를 시각화하는 코드를 추가할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type: <class 'list'>\n",
      "Number of light curves in the dataset: 10\n",
      "Structure of a single light curve data: <class 'tuple'>\n",
      "Record ID: time_series_band_1\n",
      "Time stamps tensor shape: torch.Size([99])\n",
      "Magnitude tensor shape: torch.Size([99, 1])\n",
      "Mask tensor shape: torch.Size([99, 1])\n",
      "\n",
      "Sample data from the first light curve:\n",
      "Time stamps: tensor([  1.0101,   2.0202,   3.0303,   4.0404,   5.0505,   6.0606,   7.0707,\n",
      "          8.0808,   9.0909,  10.1010,  11.1111,  12.1212,  13.1313,  14.1414,\n",
      "         15.1515,  16.1616,  17.1717,  18.1818,  19.1919,  20.2020,  21.2121,\n",
      "         22.2222,  23.2323,  24.2424,  25.2525,  26.2626,  27.2727,  28.2828,\n",
      "         29.2929,  30.3030,  31.3131,  32.3232,  33.3333,  34.3434,  35.3535,\n",
      "         36.3636,  37.3737,  38.3838,  39.3939,  40.4040,  41.4141,  42.4242,\n",
      "         43.4343,  44.4444,  45.4545,  46.4646,  47.4747,  48.4848,  49.4949,\n",
      "         50.5051,  51.5152,  52.5253,  53.5354,  54.5455,  55.5556,  56.5657,\n",
      "         57.5758,  58.5859,  59.5960,  60.6061,  61.6162,  62.6263,  63.6364,\n",
      "         64.6465,  65.6566,  66.6667,  67.6768,  68.6869,  69.6970,  70.7071,\n",
      "         71.7172,  72.7273,  73.7374,  74.7475,  75.7576,  76.7677,  77.7778,\n",
      "         78.7879,  79.7980,  80.8081,  81.8182,  82.8283,  83.8384,  84.8485,\n",
      "         85.8586,  86.8687,  87.8788,  88.8889,  89.8990,  90.9091,  91.9192,\n",
      "         92.9293,  93.9394,  94.9495,  95.9596,  96.9697,  97.9798,  98.9899,\n",
      "        100.0000], device='cuda:0')\n",
      "Magnitude: tensor([[0.0000],\n",
      "        [0.0450],\n",
      "        [0.0655],\n",
      "        [0.0553],\n",
      "        [0.0598],\n",
      "        [0.1102],\n",
      "        [0.1017],\n",
      "        [0.1016],\n",
      "        [0.1248],\n",
      "        [0.2066],\n",
      "        [0.2772],\n",
      "        [0.1849],\n",
      "        [0.2729],\n",
      "        [0.2357],\n",
      "        [0.2706],\n",
      "        [0.2717],\n",
      "        [0.3260],\n",
      "        [0.3292],\n",
      "        [0.3699],\n",
      "        [0.2832],\n",
      "        [0.2730],\n",
      "        [0.3493],\n",
      "        [0.3581],\n",
      "        [0.4460],\n",
      "        [0.4312],\n",
      "        [0.4497],\n",
      "        [0.3822],\n",
      "        [0.4656],\n",
      "        [0.5667],\n",
      "        [0.6485],\n",
      "        [0.6577],\n",
      "        [0.6816],\n",
      "        [0.6467],\n",
      "        [0.6980],\n",
      "        [0.6303],\n",
      "        [0.5346],\n",
      "        [0.5757],\n",
      "        [0.6420],\n",
      "        [0.6893],\n",
      "        [0.6422],\n",
      "        [0.6977],\n",
      "        [0.7055],\n",
      "        [0.6625],\n",
      "        [0.6362],\n",
      "        [0.7838],\n",
      "        [0.7980],\n",
      "        [0.8070],\n",
      "        [0.9117],\n",
      "        [0.8857],\n",
      "        [1.0000],\n",
      "        [0.9664],\n",
      "        [0.9109],\n",
      "        [0.8576],\n",
      "        [0.8075],\n",
      "        [0.7857],\n",
      "        [0.6966],\n",
      "        [0.7536],\n",
      "        [0.7109],\n",
      "        [0.7434],\n",
      "        [0.6989],\n",
      "        [0.6943],\n",
      "        [0.7161],\n",
      "        [0.6958],\n",
      "        [0.8258],\n",
      "        [0.8521],\n",
      "        [0.7445],\n",
      "        [0.6939],\n",
      "        [0.6048],\n",
      "        [0.6996],\n",
      "        [0.7796],\n",
      "        [0.7784],\n",
      "        [0.7322],\n",
      "        [0.7313],\n",
      "        [0.7656],\n",
      "        [0.7969],\n",
      "        [0.7722],\n",
      "        [0.7147],\n",
      "        [0.7948],\n",
      "        [0.6894],\n",
      "        [0.6888],\n",
      "        [0.6373],\n",
      "        [0.6943],\n",
      "        [0.6940],\n",
      "        [0.7622],\n",
      "        [0.7748],\n",
      "        [0.7505],\n",
      "        [0.6901],\n",
      "        [0.7007],\n",
      "        [0.6788],\n",
      "        [0.5736],\n",
      "        [0.5615],\n",
      "        [0.4770],\n",
      "        [0.4823],\n",
      "        [0.4197],\n",
      "        [0.3296],\n",
      "        [0.3261],\n",
      "        [0.3464],\n",
      "        [0.2550],\n",
      "        [0.2854]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pt_file_path = '/home/intern/SSD/intern/taejun/data_normal/lc_5.pt'\n",
    "\n",
    "# .pt 파일 로드\n",
    "light_curves_data = torch.load(pt_file_path)\n",
    "\n",
    "# 로드된 데이터의 타입과 크기 확인\n",
    "print(f\"Loaded data type: {type(light_curves_data)}\")\n",
    "print(f\"Number of light curves in the dataset: {len(light_curves_data)}\")\n",
    "\n",
    "# 첫 번째 광도곡선 데이터의 구조 확인\n",
    "first_light_curve = light_curves_data[0]\n",
    "print(f\"Structure of a single light curve data: {type(first_light_curve)}\")\n",
    "print(f\"Record ID: {first_light_curve[0]}\")\n",
    "print(f\"Time stamps tensor shape: {first_light_curve[1].shape}\")\n",
    "print(f\"Magnitude tensor shape: {first_light_curve[2].shape}\")\n",
    "print(f\"Mask tensor shape: {first_light_curve[3].shape}\")\n",
    "#print(f\"Labels tensor shape: {first_light_curve[4].shape}\")\n",
    "\n",
    "# 첫 번째 광도곡선 데이터의 일부 내용 출력\n",
    "print(\"\\nSample data from the first light curve:\")\n",
    "print(f\"Time stamps: {first_light_curve[1]}\")  # 처음 5개의 시간 스탬프\n",
    "print(f\"Magnitude: {first_light_curve[2]}\")  # 처음 5개의 광도 값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "train_size=1 should be either positive and smaller than the number of samples 0 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m total_dataset \u001b[38;5;241m=\u001b[39m train_dataset_obj[:\u001b[38;5;28mlen\u001b[39m(train_dataset_obj)]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Shuffle and split\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_selection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m record_id, tt, vals, mask, labels \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(total_dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/taejun/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/taejun/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2649\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2646\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2648\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2649\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/taejun/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2262\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_size, n_samples)\n\u001b[1;32m   2254\u001b[0m     )\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2257\u001b[0m     train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2261\u001b[0m ):\n\u001b[0;32m-> 2262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_size, n_samples)\n\u001b[1;32m   2266\u001b[0m     )\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m train_size_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for train_size: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_size))\n",
      "\u001b[0;31mValueError\u001b[0m: train_size=1 should be either positive and smaller than the number of samples 0 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "train_dataset_obj = CustomClass(root='/home/intern/SSD/intern/taejun/test_normal', train=True, preprocess=True,quantization=5,n_samples = min(800, args.n), device=device)\n",
    "\t\t# Use custom collate_fn to combine samples with arbitrary time observations.\n",
    "\t\t# Returns the dataset along with mask and time steps\n",
    "\n",
    "\n",
    "# Combine and shuffle samples from physionet Train and physionet Test\n",
    "total_dataset = train_dataset_obj[:len(train_dataset_obj)]\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle and split\n",
    "train_data, test_data = model_selection.train_test_split(total_dataset, train_size= 1, \n",
    "    random_state = 42, shuffle = True)\n",
    "\n",
    "record_id, tt, vals, mask, labels = train_data[0]\n",
    "\n",
    "n_samples = len(total_dataset)\n",
    "input_dim = vals.size(-1)\n",
    "\n",
    "batch_size = min(min(len(train_dataset_obj), args.batch_size), args.n)\n",
    "data_min, data_max = get_data_min_max(total_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size= batch_size, shuffle=False, \n",
    "    collate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"train\",\n",
    "        data_min = data_min, data_max = data_max))\n",
    "test_dataloader = DataLoader(train_data, batch_size = batch_size, shuffle=False, \n",
    "    collate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"train\",\n",
    "        data_min = data_min, data_max = data_max))\n",
    "\n",
    "attr_names = train_dataset_obj.params\n",
    "data_objects = {\"dataset_obj\": train_dataset_obj, \n",
    "            \"train_dataloader\": utils.inf_generator(train_dataloader), \n",
    "            \"test_dataloader\": utils.inf_generator(test_dataloader),\n",
    "            \"input_dim\": input_dim,\n",
    "            \"n_train_batches\": len(train_dataloader),\n",
    "            \"n_test_batches\": len(test_dataloader),\n",
    "            \"attr\": attr_names, #optional\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # `train_dataloader`에서 첫 번째 배치를 가져와서 형태와 내용을 확인합니다.\n",
    "# print(\"Train DataLoader:\")\n",
    "# for i, batch in enumerate(test_dataloader):\n",
    "#     print(f\"Batch {i}:\")\n",
    "#     print(\"observed_data.shape:\", batch['observed_data'].shape)\n",
    "#     print(\"observed_data:\", batch['observed_data'])\n",
    "#     # 여기에서 필요한 다른 키들도 확인할 수 있습니다.\n",
    "#     # 예를\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory where you want to save the file\n",
    "directory = \"experiments\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/usr/SSD/intern/taejun/latent_ode/taejun_sim_\n",
      "/home/intern/anaconda3/envs/taejun/lib/python3.8/site-packages/ipykernel_launcher.py --f=/home/intern/.local/share/jupyter/runtime/kernel-v2-3275204OMSDdZLhQHy2.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling dataset of 2 training examples\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'n_train_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 148\u001b[0m\n\u001b[1;32m    144\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(input_command)\n\u001b[1;32m    146\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamax(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr)\n\u001b[0;32m--> 148\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[43mdata_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_train_batches\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_batches \u001b[38;5;241m*\u001b[39m (args\u001b[38;5;241m.\u001b[39mniters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    151\u001b[0m \toptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'n_train_batches'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\ttorch.manual_seed(args.random_seed)\n",
    "\tnp.random.seed(args.random_seed)\n",
    "\n",
    "\texperimentID = args.load\n",
    "\tif experimentID is None:\n",
    "\t\t# Make a new experiment ID\n",
    "\t\texperimentID = int(SystemRandom().random()*100000)\n",
    "\tckpt_path = os.path.join(args.save, \"experiment_\" + str(experimentID) + '.ckpt')\n",
    "\n",
    "\tstart = time.time()\n",
    "\tprint(\"Sampling dataset of {} training examples\".format(args.n))\n",
    "\t\n",
    "\tinput_command = sys.argv\n",
    "\tind = [i for i in range(len(input_command)) if input_command[i] == \"--load\"]\n",
    "\tif len(ind) == 1:\n",
    "\t\tind = ind[0]\n",
    "\t\tinput_command = input_command[:ind] + input_command[(ind+2):]\n",
    "\tinput_command = \" \".join(input_command)\n",
    "\n",
    "\tutils.makedirs(\"results/\")\n",
    "\n",
    "\t##################################################################\n",
    "\tdata_obj = data_objects\n",
    "\tinput_dim = data_obj[\"input_dim\"]\n",
    "\n",
    "\tclassif_per_tp = False\n",
    "\tif (\"classif_per_tp\" in data_obj):\n",
    "\t\t# do classification per time point rather than on a time series as a whole\n",
    "\t\tclassif_per_tp = data_obj[\"classif_per_tp\"]\n",
    "\n",
    "\tif args.classif and (args.dataset == \"hopper\" or args.dataset == \"periodic\"):\n",
    "\t\traise Exception(\"Classification task is not available for MuJoCo and 1d datasets\")\n",
    "\n",
    "\tn_labels = 1\n",
    "\tif args.classif:\n",
    "\t\tif (\"n_labels\" in data_obj):\n",
    "\t\t\tn_labels = data_obj[\"n_labels\"]\n",
    "\t\telse:\n",
    "\t\t\traise Exception(\"Please provide number of labels for classification task\")\n",
    "\n",
    "\t##################################################################\n",
    "\t# Create the model\n",
    "\tobsrv_std = 0.01\n",
    "\tobsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
    "\tz0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))\n",
    "\n",
    "\tif args.rnn_vae:\n",
    "\t\tif args.poisson:\n",
    "\t\t\tprint(\"Poisson process likelihood not implemented for RNN-VAE: ignoring --poisson\")\n",
    "\n",
    "\t\t# Create RNN-VAE model\n",
    "\t\tmodel = RNN_VAE(input_dim, args.latents, \n",
    "\t\t\tdevice = device, \n",
    "\t\t\trec_dims = args.rec_dims, \n",
    "\t\t\tconcat_mask = True, \n",
    "\t\t\tobsrv_std = obsrv_std,\n",
    "\t\t\tz0_prior = z0_prior,\n",
    "\t\t\tuse_binary_classif = args.classif,\n",
    "\t\t\tclassif_per_tp = classif_per_tp,\n",
    "\t\t\tlinear_classifier = args.linear_classif,\n",
    "\t\t\tn_units = args.units,\n",
    "\t\t\tinput_space_decay = args.input_decay,\n",
    "\t\t\tcell = args.rnn_cell,\n",
    "\t\t\tn_labels = n_labels,\n",
    "\t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "\t\t\t).to(device)\n",
    "\t\t\n",
    "\telif args.classic_rnn:\n",
    "\t\tif args.poisson:\n",
    "\t\t\tprint(\"Poisson process likelihood not implemented for RNN: ignoring --poisson\")\n",
    "\n",
    "\t\tif args.extrap:\n",
    "\t\t\traise Exception(\"Extrapolation for standard RNN not implemented\")\n",
    "\t\t# Create RNN model\n",
    "\t\tmodel = Classic_RNN(input_dim, args.latents, device, \n",
    "\t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
    "\t\t\tn_units = args.units,\n",
    "\t\t\tuse_binary_classif = args.classif,\n",
    "\t\t\tclassif_per_tp = classif_per_tp,\n",
    "\t\t\tlinear_classifier = args.linear_classif,\n",
    "\t\t\tinput_space_decay = args.input_decay,\n",
    "\t\t\tcell = args.rnn_cell,\n",
    "\t\t\tn_labels = n_labels,\n",
    "\t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "\t\t\t).to(device)\n",
    "\t\t\n",
    "\telif args.ode_rnn:\n",
    "\t\t# Create ODE-GRU model\n",
    "\t\tn_ode_gru_dims = args.latents\n",
    "\t\t\t\t\n",
    "\t\tif args.poisson:\n",
    "\t\t\tprint(\"Poisson process likelihood not implemented for ODE-RNN: ignoring --poisson\")\n",
    "\n",
    "\t\tif args.extrap:\n",
    "\t\t\traise Exception(\"Extrapolation for ODE-RNN not implemented\")\n",
    "\n",
    "\t\tode_func_net = utils.create_net(n_ode_gru_dims, n_ode_gru_dims, \n",
    "\t\t\tn_layers = args.rec_layers, n_units = args.units, nonlinear = nn.Tanh)\n",
    "\n",
    "\t\trec_ode_func = ODEFunc(\n",
    "\t\t\tinput_dim = input_dim, \n",
    "\t\t\tlatent_dim = n_ode_gru_dims,\n",
    "\t\t\tode_func_net = ode_func_net,\n",
    "\t\t\tdevice = device).to(device)\n",
    "\n",
    "\t\tz0_diffeq_solver = DiffeqSolver(input_dim, rec_ode_func, \"euler\", args.latents, \n",
    "\t\t\todeint_rtol = 1e-3, odeint_atol = 1e-4, device = device)\n",
    "\t\n",
    "\t\tmodel = ODE_RNN(input_dim, n_ode_gru_dims, device = device, \n",
    "\t\t\tz0_diffeq_solver = z0_diffeq_solver, n_gru_units = args.gru_units,\n",
    "\t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
    "\t\t\tuse_binary_classif = args.classif,\n",
    "\t\t\tclassif_per_tp = classif_per_tp,\n",
    "\t\t\tn_labels = n_labels,\n",
    "\t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "\t\t\t).to(device)\n",
    "\telif args.latent_ode:\n",
    "\t\tmodel = create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device, \n",
    "\t\t\tclassif_per_tp = classif_per_tp,\n",
    "\t\t\tn_labels = n_labels)\n",
    "\telse:\n",
    "\t\traise Exception(\"Model not specified\")\n",
    "\n",
    "\t##################################################################\n",
    "\n",
    "\tif args.viz:\n",
    "\t\tviz = Visualizations(device)\n",
    "\n",
    "\t##################################################################\n",
    "\t\n",
    "\t#Load checkpoint and evaluate the model\n",
    "\tif args.load is not None:\n",
    "\t\tutils.get_ckpt_model(ckpt_path, model, device)\n",
    "\t\texit()\n",
    "\n",
    "\t##################################################################\n",
    "\t# Training\n",
    "\n",
    "\tlog_path = \"logs/\" + file_name + \"_\" + str(experimentID) + \".log\"\n",
    "\tif not os.path.exists(\"logs/\"):\n",
    "\t\tutils.makedirs(\"logs/\")\n",
    "\tlogger = utils.get_logger(logpath=log_path, filepath=os.path.abspath(file_name))\n",
    "\tlogger.info(input_command)\n",
    "\n",
    "\toptimizer = optim.Adamax(model.parameters(), lr=args.lr)\n",
    "\n",
    "\tnum_batches = data_obj[\"n_train_batches\"]\n",
    "\n",
    "\tfor itr in range(1, num_batches * (args.niters + 1)):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tutils.update_learning_rate(optimizer, decay_rate = 0.999, lowest = args.lr / 10)\n",
    "\n",
    "\t\twait_until_kl_inc = 10\n",
    "\t\tif itr // num_batches < wait_until_kl_inc:\n",
    "\t\t\tkl_coef = 0.\n",
    "\t\telse:\n",
    "\t\t\tkl_coef = (1-0.99** (itr // num_batches - wait_until_kl_inc))\n",
    "\n",
    "\t\tbatch_dict = utils.get_next_batch(data_obj[\"train_dataloader\"])\n",
    "\t\t# print(batch_dict)\n",
    "\t\ttrain_res = model.compute_all_losses(batch_dict, n_traj_samples = 10, kl_coef = kl_coef)\n",
    "\t\ttrain_res[\"loss\"].backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tn_iters_to_viz = 10\n",
    "\t\tif itr % (n_iters_to_viz * num_batches) == 0:\n",
    "\t\t\twith torch.no_grad():\n",
    "\n",
    "\t\t\t\ttest_res = compute_loss_all_batches(model, \n",
    "\t\t\t\t\tdata_obj[\"test_dataloader\"], args,\n",
    "\t\t\t\t\tn_batches = data_obj[\"n_test_batches\"],\n",
    "\t\t\t\t\texperimentID = experimentID,\n",
    "\t\t\t\t\tdevice = device,\n",
    "\t\t\t\t\tn_traj_samples = 10, kl_coef = kl_coef)\n",
    "\n",
    "\t\t\t\tmessage = 'Epoch {:04d} [Test seq (cond on sampled tp)] | Loss {:.6f} | Likelihood {:.6f} | KL fp {:.4f} | FP STD {:.4f}|'.format(\n",
    "\t\t\t\t\titr//num_batches, \n",
    "\t\t\t\t\ttest_res[\"loss\"].detach(), test_res[\"likelihood\"].detach(), \n",
    "\t\t\t\t\ttest_res[\"kl_first_p\"], test_res[\"std_first_p\"],\n",
    "\t\t\t\t\ttrain_res['loss'].detach(),train_res['likelihood'].detach())\n",
    "\t\t\t\tsample_test = utils.get_next_batch(data_obj[\"test_dataloader\"])\n",
    "\t\t\t\tno_zero_count = torch.sum(sample_test['observed_data']!= 0).item()\n",
    "\t\t\t\tzero_count = torch.sum(sample_test['observed_data'] == 0).item()\n",
    "\t\t\t\tprint(\"zero_count 개수: \",zero_count)\n",
    "\t\t\t\tprint(\"no_zero_count 개수: \",no_zero_count)\n",
    "\t\t\t\tplot_name_test = \"test_2024_02_15_resultFinal_test{:04d}\".format(itr//num_batches)\n",
    "\t\t\t\tplot_name_train = \"train_2024_02_15_resultFinal_train{:04d}\".format(itr//num_batches)\n",
    "\t\t\t\tVisualizations(device).draw_all_plots_one_dim(sample_test ,model.to(device), plot_name=plot_name_test, save = True)\n",
    "\t\t\t\tVisualizations(device).draw_all_plots_one_dim(batch_dict ,model.to(device), plot_name=plot_name_train, save = True)\n",
    "\t\t\t\t\n",
    "\t\t \t\n",
    "\t\t\t\tlogger.info(\"Experiment \" + str(experimentID))\n",
    "\t\t\t\tlogger.info(message)\n",
    "\t\t\t\tlogger.info(\"KL coef: {}\".format(kl_coef))\n",
    "\t\t\t\tlogger.info(\"Train loss (one batch): {}\".format(train_res[\"loss\"].detach()))\n",
    "\t\t\t\tlogger.info(\"Train CE loss (one batch): {}\".format(train_res[\"ce_loss\"].detach()))\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\tif \"mse\" in test_res:\n",
    "\t\t\t\t\tlogger.info(\"Test MSE: {:.4f}\".format(test_res[\"mse\"]))\n",
    "\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\t\t\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n",
      "zero_count 개수:  1015\n",
      "no_zero_count 개수:  740\n",
      "shape torch.Size([10, 1, 100, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 89815\n",
      "Epoch 0993 [Test seq (cond on sampled tp)] | Loss 370.338989 | Likelihood -481.314362 | KL fp 3.4721 | FP STD 0.5245|\n",
      "KL coef: 0.9999487851046791\n",
      "Train loss (one batch): 73.70389556884766\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0970\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    test_res = compute_loss_all_batches(model, \n",
    "        data_obj[\"test_dataloader\"], args,\n",
    "        n_batches = data_obj[\"n_test_batches\"],\n",
    "        experimentID = experimentID,\n",
    "        device = device,\n",
    "        n_traj_samples = 5, kl_coef = kl_coef)\n",
    "\n",
    "    message = 'Epoch {:04d} [Test seq (cond on sampled tp)] | Loss {:.6f} | Likelihood {:.6f} | KL fp {:.4f} | FP STD {:.4f}|'.format(\n",
    "        itr//num_batches, \n",
    "        test_res[\"loss\"].detach(), test_res[\"likelihood\"].detach(), \n",
    "        test_res[\"kl_first_p\"], test_res[\"std_first_p\"])\n",
    "    sample_test = utils.get_next_batch(data_obj[\"test_dataloader\"])\n",
    "    no_zero_count = torch.sum(sample_test['observed_data']!= 0).item()\n",
    "    zero_count = torch.sum(sample_test['observed_data'] == 0).item()\n",
    "    print(\"zero_count 개수: \",zero_count)\n",
    "    print(\"no_zero_count 개수: \",no_zero_count)\n",
    "    plot_name = \"test_2024_02_15_10_{:04d}\".format(itr//num_batches)\n",
    "    viz.draw_all_plots_one_dim(sample_test ,model.to(device), plot_name=plot_name, save = True)\n",
    "    \n",
    "\n",
    "    logger.info(\"Experiment \" + str(experimentID))\n",
    "    logger.info(message)\n",
    "    logger.info(\"KL coef: {}\".format(kl_coef))\n",
    "    logger.info(\"Train loss (one batch): {}\".format(train_res[\"loss\"].detach()))\n",
    "    logger.info(\"Train CE loss (one batch): {}\".format(train_res[\"ce_loss\"].detach()))\n",
    "    \n",
    "    \n",
    "\n",
    "    if \"mse\" in test_res:\n",
    "        logger.info(\"Test MSE: {:.4f}\".format(test_res[\"mse\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mtrain_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m      2\u001b[0m train_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mtrain_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mviz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_all_plots_one_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/usr/SSD/intern/taejun/latent_ode/lib/plotting.py:361\u001b[0m, in \u001b[0;36mVisualizations.draw_all_plots_one_dim\u001b[0;34m(self, data_dict, model, plot_name, save, experimentID)\u001b[0m\n\u001b[1;32m    354\u001b[0m plot_trajectories(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39max_traj[traj_id], \n\u001b[1;32m    355\u001b[0m \tdata_for_plotting[traj_id]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), observed_time_steps, \n\u001b[1;32m    356\u001b[0m \tmask \u001b[38;5;241m=\u001b[39m mask_for_plotting[traj_id]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    357\u001b[0m \tmin_y \u001b[38;5;241m=\u001b[39m min_y, max_y \u001b[38;5;241m=\u001b[39m max_y, \u001b[38;5;66;03m#title=\"True trajectories\", \u001b[39;00m\n\u001b[1;32m    358\u001b[0m \tmarker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, dim_to_show \u001b[38;5;241m=\u001b[39m dim_to_show,\n\u001b[1;32m    359\u001b[0m \tcolor \u001b[38;5;241m=\u001b[39m cmap(\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# Plot reconstructions\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m \u001b[43mplot_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43max_traj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtraj_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mreconstructions_for_plotting\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtraj_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps_to_predict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmin_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSample \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m (data space)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraj_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_to_show\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdim_to_show\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m\t\u001b[49m\u001b[43madd_to_plot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# Plot variance estimated over multiple samples from approx posterior\u001b[39;00m\n\u001b[1;32m    366\u001b[0m plot_std(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39max_traj[traj_id], \n\u001b[1;32m    367\u001b[0m \treconstructions_for_plotting[traj_id]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), reconstr_std[traj_id]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), \n\u001b[1;32m    368\u001b[0m \ttime_steps_to_predict, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, color \u001b[38;5;241m=\u001b[39m cmap(\u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m/media/usr/SSD/intern/taejun/latent_ode/lib/plotting.py:75\u001b[0m, in \u001b[0;36mplot_trajectories\u001b[0;34m(ax, traj, time_steps, min_y, max_y, title, add_to_plot, label, add_legend, dim_to_show, linestyle, marker, mask, color, linewidth)\u001b[0m\n\u001b[1;32m     72\u001b[0m \tax\u001b[38;5;241m.\u001b[39mset_ylim(top \u001b[38;5;241m=\u001b[39m max_y)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(traj\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 75\u001b[0m \td \u001b[38;5;241m=\u001b[39m \u001b[43mtraj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:, dim_to_show] \u001b[38;5;66;03m# 수정\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \tts \u001b[38;5;241m=\u001b[39m time_steps\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     77\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "\n",
    "viz.draw_all_plots_one_dim(batch_dict ,model.to(device), plot_name=plot_name, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traj_from_prior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtraj_from_prior\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traj_from_prior' is not defined"
     ]
    }
   ],
   "source": [
    "print(traj_from_prior.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 523, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dict['observed_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object inf_generator at 0x7fcfd36ec190>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_obj[\"test_dataloader\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taejun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
